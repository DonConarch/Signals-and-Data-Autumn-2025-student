{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "\n",
    "# Week 9 - Word Embeddings\n",
    "\n",
    "Word embeddings is a very popular an effective manner of giving ML models a sort of background knowledge of the vocabulary of a given document corpus. \n",
    "\n",
    "A word embedding is effectively, just a learned vector representation for text. The vectors can be in any number of arbitrary dimensions $m$, but the rule of thumb is often, that larger vocabularies and more intricate patterns, require more dimensions. Often, (and in this case) the goal is to have the vector representations model words semantic meanings, so words with similar meaning in texts, have similar representations. An example could be the words \"cat\" and \"mouse\" having a small difference between them, when compared to a word such as \"prestidigitation\".\n",
    "\n",
    "Two things in particular are cool about word embeddings:\n",
    "\n",
    "1. They can be used for downstream tasks; if you make a set of word embeddings using one dataset, they can most likely be used for other NLP applications, not even relevant to the original dataset on which the embeddings were trained.\n",
    "2. They often \"naturally\" appear in neural networks. The first few layers often create useful representations of words so that similar words have similar representations. This means, these initial neural network layers can be extracted for other downstream applications, something which [Skipgram and CBOW](https://medium.com/@RobuRishabh/learning-word-embeddings-with-cbow-and-skip-gram-b834bde18de4) models leverage.\n",
    "\n",
    "---\n",
    "\n",
    "**At the end of this week, you should be able to:**\n",
    "\n",
    "- Explain what word embeddings are\n",
    "- Explain what a word co-occurence matrix is\n",
    "- Know how to compare words using cosine similarities\n",
    "- Explain hoa  GloVe word-embedding matrix looks roughly\n",
    "- Explain how word embedding vectors relate to one another for related words.\n",
    "- Reflect on how word embeddings can transmit societal bias \n",
    "\n",
    "---\n",
    "\n",
    "## GloVE\n",
    "\n",
    "[GloVe: Gloval Vectors for Word Representations](https://nlp.stanford.edu/projects/glove/) is an unsupervised learning method to obtain vector representations for words. [The paper is available here](https://nlp.stanford.edu/pubs/glove.pdf). The group out of Stanford, have released numerous different models, we will work with one trained on Wikipedia 2014, which contains $400000$ words, each represented by a feature vector with a dimensionality of $50$. You can download the full vector dataset here: [http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "\n",
    "GloVe wasn't the first word embedding model, and today, is probably not even the best. What was unique about them, was the way they mixed *local* and *global* representations.\n",
    "\n",
    "With bag-of-words and LSA in week 7, for example, we only achived a global representation, words related if they were in the same document, no matter how far apart they were. In Skipgram and CBOW (as fasttext uses), only local contexts matter. Whether words share a document does not matter, so long as they do not share a local context. GloVe, via \"dark magic mathematics\" (not necessary to learn in-depth), mixes both of these. Moreover, they constrain their embeddings to have linear relationships between a lot of the words:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "  <img src=\"images/linear_relationships.png\" alt=\"Centered Image\"/>\n",
    "</div>\n",
    "\n",
    "Meaning, simple additive or multiplicative operations should bring you somewhat closer to another word. To achieve this, they among other things, eschew the use of Neural networks to create the embeddings, as they are by design, nonlinaer. Meaning, even if they led to better embeddings, the relationship between different embeddings, could be decidedly nonlinear, and difficult to interpret by humans.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import spatial\n",
    "from scipy.linalg import eigh\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 0 - The expected preprocessing step\n",
    "\n",
    "The first thing we wanna do to help the understanding of what GloVe does, is make a co-occurence matrix. You'll remember similar from week 8, which is simply where we check how often words appear in each other's contexts.\n",
    "\n",
    "#### **1.1. Briefly inspect the `preprocess_text` function below, and list what it does:**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:30:56.927507Z",
     "start_time": "2024-11-01T12:30:56.918204Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?! \\n]+\", \"\", text)\n",
    "    text = text.split(\"\\n\")\n",
    "    text = [line.strip() for line in text if line.find(\"chapter\") == -1]\n",
    "    text = \"\\n\".join(text)\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:31:34.880165Z",
     "start_time": "2024-11-01T12:31:34.856125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"data/Reviews.csv\")\n",
    "reviews = data[\"Text\"].values[:4] # take only first couple of reviews\n",
    "\n",
    "# Clean the data using our function above\n",
    "reviews_clean = [preprocess_text(review) for review in reviews]\n",
    "\n",
    "# Split each word in lists\n",
    "words_in_review = []\n",
    "for review in reviews_clean:\n",
    "    words_in_review.extend(review.split())\n",
    "print(words_in_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 1 - Co-occurrence matrix\n",
    "\n",
    "The first thing we wanna do to help the understanding of what GloVe does, is make a co-occurence matrix. You'll remember similar from week 8, which is simply where we check how often words appear in each other's contexts.\n",
    "\n",
    "\n",
    "#### **1.1 Look at the `calculate_co_occurence` function, and get a feel for how it works, see if you can locate all the given steps:**\n",
    "\n",
    "1. Initialise a default dictionary with the Counter object\n",
    "2. Iterate across each review and then enumerate across words in that review\n",
    "3. Calculate at what index to start and stop counting words based $L$\n",
    "    - Make sure you do not count the word itself\n",
    "4. Increment the counter for the context of the word under consideration\n",
    "    - `co_occurrence_counts[word][words[j]] += 1`\n",
    "5. Convert the dictionary to a pandas dataframe\n",
    "    - `pd.DataFrame(co_occurrence_counts).fillna(0)` does the job!\n",
    "6. Sort the rows and columns by their respective indexes\n",
    "    - `.sort_index(axis=0).sort_index(axis=1)`\n",
    "\n",
    "#### **1.2 Run the code two cells below, the resulting plot should be how often words appear in each other's contexts. Describe if anything stands out.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:32:56.609321Z",
     "start_time": "2024-11-01T12:32:56.553200Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_co_occurence(texts, L_front, L_end=None):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (_type_): The text to calculate the co-occurence matrix of\n",
    "        L_front (int): Number of words before to consider\n",
    "        L_end (int), optional): Number of words after a word to consider, if None is the same as L_front\n",
    "    \"\"\"\n",
    "    if L_end is None:\n",
    "        L_end = L_front\n",
    "    \n",
    "    # Initialize an empty dictionary for co-occurrence counts\n",
    "    # collections.Counter in this case, is a more 'powerful' alternative to defaultdict(lambda: 0)\n",
    "    co_occurrence_counts = defaultdict(Counter)\n",
    "\n",
    "    # Build co-occurrence counts based on reviews\n",
    "    for text in texts:\n",
    "        words = text.lower().split()\n",
    "        for i, word in enumerate(words):\n",
    "            # Look L words before and after the current word\n",
    "            start = max(0, i - L_front)\n",
    "            end = min(len(words), i + L_end + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    co_occurrence_counts[word][words[j]] += 1\n",
    "\n",
    "    # Convert to DataFrame for better readability\n",
    "    X_ij = pd.DataFrame(co_occurrence_counts).fillna(0)\n",
    "    X_ij = X_ij.sort_index(axis=0).sort_index(axis=1)\n",
    "    return X_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plot $X$ as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:33:00.368107Z",
     "start_time": "2024-11-01T12:32:59.428909Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_ij = calculate_co_occurence(reviews, 1)\n",
    "print(X_ij)\n",
    "\n",
    "plt.imshow(X_ij.to_numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 2 Load Glove Vectors \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 2 - Loading GloVe vectors \n",
    "\n",
    "The Glove vectors we are going to use are located in a text file, where each line describes one word and its associated feature vector. Formatted as such: `word float float float ...`\n",
    "\n",
    "#### **ðŸ’» 2.1 Implement the `load_glove_vectors` function below, it should:**\n",
    "\n",
    "1. Open the file a given glove file\n",
    "2. Iterate across each line\n",
    "3. Split the line into the word and the vector\n",
    "    - Numpy's `np.asarray()` function can take a string containing numbers and put them in an array\n",
    "4. Insert the word as a key in the dictionary and the vector as the value\n",
    "\n",
    "\n",
    "#### **2.2 If we represented the GlOVe dictionary as one big matrix, what would its dimensionality be, and what would this mean?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:46:03.400137Z",
     "start_time": "2024-10-30T09:45:56.759006Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file_path):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "    return glove_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:48:57.281961Z",
     "start_time": "2024-10-30T10:48:57.272918Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"glove.6B.50d.txt\"\n",
    "glove_dictionary = load_glove_vectors(filename)\n",
    "\n",
    "print(len(glove_dictionary), glove_dictionary[\"have\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 3 - Word similarities\n",
    "\n",
    "In this exercise you will inspect the word embedding for different words, in order to determine whether words that semantically have similar meaning (or are semantically related) also have similar representations under the embedding.\n",
    "\n",
    "To compare two words, we use the cosine similarity score between the feature vectors representing the two words.\n",
    "\n",
    "#### **ðŸ’» 3.1. Implement a function `cos_sim`, which computes the cosine similarity of two vectors: $\\text{cosine\\_similarity} = 1 - \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|}$**\n",
    "\n",
    "#### **ðŸ’» 3.2 Implement the function `word_similarity`, which computes the similarity of two words as strings.**\n",
    "\n",
    "#### **3.3 Run the cells after two steps below, and compare the simliarity scores for each word pair within each group. You can use the following questions as guide for your answer:**\n",
    "\n",
    "- Do the relative differences in scores reflect your own intuition? \n",
    "- Does this inform you about certain relations captured by the word embedding? \n",
    "- For instance, what can you learn from observing that 'Denmark' is more similar to 'Sweden' than to 'Copenhagen'?\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **3.4 What would be the similarity between the words \"cat\" and \"cat\"?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:29:31.144464Z",
     "start_time": "2024-10-30T10:29:31.124430Z"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "def word_similarity(word1, word2, verbose=True):    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:46:03.806258Z",
     "start_time": "2024-10-30T09:46:03.787260Z"
    }
   },
   "outputs": [],
   "source": [
    "#humans and family structures\n",
    "word_similarity(\"king\",\"queen\")\n",
    "word_similarity(\"prince\",\"princess\")\n",
    "word_similarity(\"prince\",\"brother\")\n",
    "word_similarity(\"princess\",\"sister\")\n",
    "word_similarity(\"prince\",\"sister\")\n",
    "word_similarity(\"princess\",\"brother\")\n",
    "word_similarity(\"brother\",\"sister\")\n",
    "word_similarity(\"brother\",\"son\")\n",
    "word_similarity(\"sister\",\"daughter\")\n",
    "word_similarity(\"brother\",\"daughter\")\n",
    "word_similarity(\"sister\",\"son\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:46:03.882848Z",
     "start_time": "2024-10-30T09:46:03.868140Z"
    }
   },
   "outputs": [],
   "source": [
    "#prepositions and orientations\n",
    "word_similarity(\"after\",\"before\")\n",
    "word_similarity(\"over\",\"under\")\n",
    "word_similarity(\"vertical\",\"horizontal\")\n",
    "word_similarity(\"left\",\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:46:03.946353Z",
     "start_time": "2024-10-30T09:46:03.925353Z"
    }
   },
   "outputs": [],
   "source": [
    "#toponyms (places)\n",
    "word_similarity(\"city\",\"country\")\n",
    "word_similarity(\"city\",\"town\")\n",
    "word_similarity(\"denmark\",\"sweden\")\n",
    "word_similarity(\"denmark\",\"copenhagen\")\n",
    "word_similarity(\"stockholm\",\"copenhagen\")\n",
    "word_similarity(\"stockholm\",\"denmark\")\n",
    "word_similarity(\"sweden\",\"stockholm\")\n",
    "word_similarity(\"sweden\",\"copenhagen\")\n",
    "word_similarity(\"walk\",\"run\")\n",
    "word_similarity(\"run\",\"sit\")\n",
    "word_similarity(\"walk\",\"sit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:46:04.008006Z",
     "start_time": "2024-10-30T09:46:03.992904Z"
    }
   },
   "outputs": [],
   "source": [
    "#Homonyms (words with different meanings)\n",
    "word_similarity(\"second\",\"minute\")\n",
    "word_similarity(\"first\",\"second\")\n",
    "word_similarity(\"first\",\"minute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 4: Nearest Neighbours\n",
    "\n",
    "It can be pretty useful to find the $N$ nearest neighbours to a word, given the glove embedding.\n",
    "\n",
    "#### **ðŸ’» 4.1 Implement the function `nearest_neighbours` which computes the $N$ nearest neighbours (most similar words) to a single word. You can follow the steps as outlined below:**\n",
    "\n",
    "1. Create an array containing all the vectors representations of every word and one for the vectors\n",
    "         - `.keys()` provides an iterable of the keys of a dictionary\n",
    "         - `.values()` does the same for the values, in the same order \n",
    "2. Compute the similarity scores using the dot product between the vector of the word and all the words in the vocabulary\n",
    "3. Find the top indices \n",
    "\n",
    "#### **4.2 Look at the various words two steps below, do you agree that they are similar?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.3 Try to come up with some words that are dissimilar from any other What do you find?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:46:04.053912Z",
     "start_time": "2024-10-30T09:46:04.039911Z"
    }
   },
   "outputs": [],
   "source": [
    "def nearest_neighbours(word, num_neighbours):    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:46:04.975466Z",
     "start_time": "2024-10-30T09:46:04.084431Z"
    }
   },
   "outputs": [],
   "source": [
    "nearest_neighbours(\"denmark\", 3)\n",
    "nearest_neighbours(\"king\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 5 - Visualizing words using PCA\n",
    "\n",
    "In this exercise we use principal component analysis (PCA) to reduce the dimensionality of the feature vector space. This allows us to illustrate the spatial relation between different words from the vocabulary.\n",
    "\n",
    "#### **5.1. ðŸ’» Implement the function, `plot_pca` that conducts PCA on a sub-set of the data and projects these data-points onto the first two principal components. You can follow these steps:** \n",
    "- Follow these steps:\n",
    "    1. Obtain a matrix of the vectors and transpose it\n",
    "    2. Subtract the mean of each dimension (axis=1) from the matrix\n",
    "    3. Compute the eigenvectors and values of the covariance matrix\n",
    "    4. Project the data onto the eigenvectors. Pay attention to the dimensions in order to do the projection correctly\n",
    "    5. The eigenvectors are initially sorted in ascending order, take use the dimensions corresponding to the largest eigenvalues and plot them\n",
    "\n",
    "#### **5.2 Comment on any linearity and distance between pairs of words.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **5.3 When visualizing for many words, comment on which words are grouped together. Does the grouping capture any semantic meaning?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:12:36.779554Z",
     "start_time": "2024-10-30T10:12:36.756555Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca(word_list):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:12:39.310645Z",
     "start_time": "2024-10-30T10:12:37.436682Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_pca([\"ship\",\"harbor\",\"car\",\"garage\"])\n",
    "plot_pca([\"father\",\"mother\",\"son\",\"daughter\"])\n",
    "plot_pca([\"king\",\"queen\",\"prince\",\"princess\",\"man\",\"woman\",\"boy\",\"girl\"])\n",
    "plot_pca([\"denmark\",\"copenhagen\",\"sweden\",\"stockholm\",\"germany\",\"berlin\",\"france\",\"paris\"])\n",
    "plot_pca([\"guitar\",\"piano\",\"saxophone\",\"cat\",\"dog\",\"horse\",\"tiger\",\"book\",\"article\",\"journal\",\"computer\",\"telephone\",\"dishwasher\",\"gramaphone\",\"house\",\"garden\",\"lawn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 6 - Solving analogies\n",
    "As defined by Aristotle as \"an equality of proportions\", an analogy can be considered a problem involving four terms, such that the second term is related to the first in a similar way as the forth term is related to the third.\n",
    "\n",
    "An example of an analogy is;\n",
    "- *evening* is to *morning* as *dinner* is to *breakfast*\n",
    "\n",
    "Analogies are often used as verbal reasoning tests, where the test subject is asked to infer the similarity between two relations, by only being presented the first three terms of the analogy problem.\n",
    "For instance, the subject is expected to infer the term *girl* from the problem;\n",
    "- *brother* is to *sister* as *boy* is to *???*\n",
    "\n",
    "These kind of problems are often included in assessments of intelligence (such as IQ or children development tests) as the ability to draw analogies are believed to reflect the underlying cognitive ability to represent and reason about higher-order relations.\n",
    "\n",
    "In this exercise you are going to examine an AI algorithm for solving such analogy problems. As input, the algorithm is given the first three terms of an analogy problem. It will then rely on the linear substructure of the GloVe word embedding to find and return the best candidate word for the forth term.\n",
    "\n",
    "**Algorithm:**\n",
    "For a given problem, let $v_1,v_2,v_3$ be the vector representations for the first three terms. \n",
    "The guess for the forth term is found as the word for which the associated vector representation is most similar to the vector $v_d = (v_3 - (v_1-v_2))$. To compare vectors, the algorithm uses the cosine similarity measure.\n",
    "\n",
    "#### **6.1. Argue why this approach is reasonable and why $v_d$ is computed as it is**\n",
    "\n",
    "*Hint: It may be easier to build intuition from considering the problem in 2D.*\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **ðŸ’» 6.2 Implement the function `find_analogy`, you can follow the following steps:**\n",
    "\n",
    "- Follow these steps:\n",
    "    1. Get the vector representations for each of the 3 words and calculate the fourth vector\n",
    "    2. Create a matrix of word vectors that excludes the three original words \n",
    "    3. Calculate the distances to $v_d$\n",
    "        - For this, implement a cos_sim function that calculates the cosine similarity between a vector and a matrix `cos_sim_vec_mat`\n",
    "    4. Return the `N` closest words and their scores\n",
    "        - `np.argsort` sorts a vector in ascending order, `[::-1]` can be used to reverse the order of an array\n",
    "        - To use the indexes on a list it must be converted to a numpy array\n",
    "\n",
    "#### **6.3 Test the algorithm with the provided examples as well as with analogy problems of your own. Investigate whether there are particular types of analogies that the algorithm cannot solve. Comment on why this is.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **ðŸ’» 6.4 For real cognitive tests the analogy problem is often presented as a multiple-choice question. Implement the function `find_analogy_multiple_choice` to present such a setting. Before you start, think on whether you would expect it to perform better or worse, and test it afterwards. The changes that need to be make programatically, are:**\n",
    "\n",
    " 1. The function must take a list of words from which to choose \n",
    " 2. These must be used to create the matrix for calculating the similarities\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:12:41.191879Z",
     "start_time": "2024-10-30T10:12:41.167878Z"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim_vec_mat(v1, v2):\n",
    "    \"\"\"Compute cosine similarity between a vector and a matrix.\"\"\"\n",
    "    # Normalize the vector v1\n",
    "    v1_norm = v1 / np.linalg.norm(v1)\n",
    "\n",
    "    # Normalize the matrix v2\n",
    "    v2_norm = v2 / np.linalg.norm(v2, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = np.dot(v2_norm, v1_norm)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def find_analogy(w1, w2, w3, N=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:13:36.711535Z",
     "start_time": "2024-10-30T10:13:35.219534Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "#examples of analogy problems\n",
    "\n",
    "#w1 = \"man\"; w2 = \"woman\"; w3 = \"king\";\n",
    "#w1 = \"brother\"; w2 = \"sister\"; w3 = \"boy\";\n",
    "#w1 = \"pen\"; w2 = \"paper\"; w3 = \"doctor\";\n",
    "#w1 = \"denmark\"; w2 = \"copenhagen\"; w3 = \"germany\";\n",
    "#w1 = \"denmark\"; w2 = \"copenhagen\"; w3 = \"france\";\n",
    "#w1 = \"denmark\"; w2 = \"france\"; w3 = \"copenhagen\";\n",
    "#w1 = \"car\"; w2 = \"parking\"; w3 = \"ship\";\n",
    "#w1 = \"left\"; w2 = \"right\"; w3 = \"horizontal\";\n",
    "#w1 = \"woman\"; w2 = \"man\"; w3 = \"mother\";\n",
    "w1 = \"man\"; w2 = \"woman\"; w3 = \"doctor\";\n",
    "\n",
    "w4, scores = find_analogy(w1, w2, w3, N)\n",
    "print(w1, \"is to\", w2, \"as\", w3, \"is to:\")\n",
    "for i, (word, score) in enumerate(zip(w4, scores)):\n",
    "    print(f\"{i+1}: {word}: {score:.3}\")\n",
    "plot_pca([w1,w2,w3,*w4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:23:31.365913Z",
     "start_time": "2024-10-30T10:23:31.341909Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_analogy_multiple_choice(w1, w2, w3, choices, N=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:23:32.244200Z",
     "start_time": "2024-10-30T10:23:31.852434Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 4\n",
    "# w1 = \"man\"; w2 = \"woman\"; w3 = \"king\"; options = [\"queen\", \"prince\", \"father\", \"sister\"]\n",
    "# w1 = \"brother\"; w2 = \"sister\"; w3 = \"boy\"; options = [\"girl\", \"man\", \"child\", \"parent\"]\n",
    "# w1 = \"pen\"; w2 = \"paper\"; w3 = \"doctor\"; options = [\"stethoscope\", \"hospital\", \"patient\", \"nurse\"]\n",
    "# w1 = \"denmark\"; w2 = \"copenhagen\"; w3 = \"germany\"; options = [\"berlin\", \"france\", \"sweden\", \"norway\"]\n",
    "# w1 = \"denmark\"; w2 = \"copenhagen\"; w3 = \"france\"; options = [\"paris\", \"lyon\", \"marseille\", \"nice\"]\n",
    "# w1 = \"denmark\"; w2 = \"france\"; w3 = \"copenhagen\"; options = [\"paris\", \"stockholm\", \"oslo\", \"helsinki\"]\n",
    "# w1 = \"car\"; w2 = \"parking\"; w3 = \"ship\"; options = [\"dock\", \"garage\", \"boat\", \"plane\"]\n",
    "# w1 = \"left\"; w2 = \"right\"; w3 = \"horizontal\"; options = [\"vertical\", \"diagonal\", \"up\", \"down\"]\n",
    "# w1 = \"woman\"; w2 = \"man\"; w3 = \"mother\"; options = [\"father\", \"child\", \"sister\", \"parent\"]\n",
    "w1 = \"man\"; w2 = \"woman\"; w3 = \"doctor\"; options = [\"nurse\", \"teacher\", \"engineer\", \"lawyer\"]\n",
    "\n",
    "w4, scores = find_analogy_multiple_choice(w1, w2, w3, choices=options, N=N)\n",
    "print(w1, \"is to\", w2, \"as\", w3, \"is to:\")\n",
    "for i, (word, score) in enumerate(zip(w4, scores)):\n",
    "    print(f\"{i+1}: {word}: {score:.3}\")\n",
    "plot_pca([w1,w2,w3,*w4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 7 - Quantifying stereotypes\n",
    "\n",
    "As for most machine learning algorithms, word embedding can be proned to biases. If the original data contained systematic biases, the learned representations may also reflect these biases. The GloVe machine learning algorithm was trained on Wikipedia texts. If these texts in general contained or described specific stereotypes, the word embedding will also capture these stereotypes. \n",
    "Often it is found that word embeddings contain gender stereotypes, often significantly with regards to professions.\n",
    "\n",
    "#### **7.1. In the above exercises we have used the plot_pca function to visualize the words \"nurse\", \"doctor\", \"man\" and \"woman\". Comment on the plots in terms of gender stereotypes.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **8.2 Compare the cosine distance between various professions (\"programmer\", \"secretary\", \"pilot\", \"president\", \"nurse\",...) and the words \"mother\" and \"farther\". Comment on your findings.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **8.3 Evaluate terms that relate to the two words \"man\" and \"woman\" by comparing the nearest neighbours to \"man\" and \"woman\" respectively. Are there any professions related to \"man\" and \"woman\"? Do \"man\" and \"woman\" have neighbours in common?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **8.4 Based on your findings, discuss whether you can use the analogy solver to test for various biases or stereotypes that may have been conveyed to the word embedding**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **8.5. Reflect on whether word embedding in general is a feasible approach to verify if a text corpus contains biases or stereotypes.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:33:11.928952Z",
     "start_time": "2024-10-30T10:33:10.981906Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Define the words for which you want to compute similarities\n",
    "words = [\"man\", \"woman\", \"programmer\", \"secretary\", \"pilot\", \"president\", \"nurse\", \"engineer\",]\n",
    "\n",
    "# Create a similarity matrix\n",
    "similarity_matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "for i, word1 in enumerate(words):\n",
    "    for j, word2 in enumerate(words):\n",
    "        similarity_matrix[i, j] = word_similarity(word1, word2, verbose=False)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_df, annot=True, cmap='coolwarm', fmt='.2f', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Word Similarity Heatmap')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signals-and-data-autumn-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
