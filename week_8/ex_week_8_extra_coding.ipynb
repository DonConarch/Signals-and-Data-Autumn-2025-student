{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73d30a8",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "# Week 8 - NLP, N-grams and FastText\n",
    "\n",
    "As you have seen in the lectures, NLP has a wide range of techniques and applications of such techniques. We will give you an introduction to some of these techniques, and today you will get hands-on experience with them. In today's exercise, we will look at the following topics:\n",
    "\n",
    "1. How do we represent text in a vectorized way that encodes context? (One answer here is N-grams, and those we will look at).\n",
    "2. How do we create and sample from an N-gram language model - and how does the size of the grams affect the generated text?\n",
    "3. How do we use a pre-existing language model (FastText), to classify text messages as spam?\n",
    "\n",
    "The data we will be using later today is a dataset consisting of \"spam or ham\" text messages. The dataset consists of a number of text messages, some of which are spam and some of which are so-called \"ham\". We will use FastText to classify mails as spam or ham. For now, we will be looking at some different texts, to see how we can use N-grams to generate text, and how we can create N-gram language models from a text corpus.\n",
    "\n",
    "---\n",
    "\n",
    "**At the end of this week, you should be able to:**\n",
    "\n",
    "- Know what N-grams mean\n",
    "- Know how word-wise and character-wise N-grams can be constructed\n",
    "- Know how to model next-word probabilities given a context\n",
    "- Know how to model next-n-gram probabilities given a context\n",
    "- Know roughly how Fasttext uses n-grams and character-grams\n",
    "\n",
    "**Optional:**\n",
    "\n",
    "- Know how to implement your own fasttext model\n",
    "- Know how to fiddle around with parameters in the fasttext library\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **N-grams and word probability models:**\n",
    "\n",
    "Recall we want to use the N-grams for probabilistic word modelling tasks, for example, next word predictions given some sequence of words which we can express the following way:\n",
    "\n",
    "$$\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1})\n",
    "$$\n",
    "The problem is that estimating such probabilities for very long sequences is computationally and memory-wise VERY expensive. So as a solution we sometimes use N-grams. In N-grams, the assumption is that we can model these conditional dependencies with shorter sequences of words, i.e.:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n) \\quad \\text{(Unigram)}\\\\\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n| w_{n-1}) \\quad \\text{(Bigram)}\\\\\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n|w_{n-2}, w_{n-1}) \\quad \\text{(Trigram)}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Which we then compute as:\n",
    "\n",
    "$$\n",
    "P(w_n|w_{n-2}, w_{n-1}) = \\frac{\\text{Count}(w_{n-2}, w_{n-1}, w_n)}{\\text{Count}(w_{n-2}, w_{n-1})}\n",
    "$$\n",
    "\n",
    "The language model that we create is based on some text corpus from which we obtain the count measures.\n",
    "\n",
    "\n",
    "**Fasttext (briefly):**\n",
    "\n",
    "Fasttext is a model created by Meta (back when they were still facebook). Basically, it is a [CBoW or SkipGram model](https://fasttext.cc/docs/en/unsupervised-tutorial.html#Advanced%20readers:%20skipgram%20versus%20cbow), which uses **both character and word n-grams**.\n",
    "\n",
    "CBoWs and Skipgrams are 'nothing special', they're quite ubiquitous, especially around the time of fasttext, what set it apart was a lot of clever software tricks, as well as its use of n-grams. This is arguably what you should focus on learning about it, if anything.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f31431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:13.135997Z",
     "start_time": "2024-10-28T08:31:05.754704Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b869382",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 1: N-grams and word probabilites\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74044819",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 1: Text-loading\n",
    "\n",
    "\n",
    "The texts we will be experimenting with N-grams on are the two famous books Pride & Prejudice by Jane Austen and The Origin of Species by Charles Darwin. The two books have been obtained in a raw text format from https://www.gutenberg.org/, i.e. Project Gutenberg which concerns itself with the collection of Open Access e-books.\n",
    "\n",
    "A big part of working with text documents is unfortunately having to preprocess the documents. Preprocessing of these, can have a large impact on the eventual performance of language models, such as N-gram models. We have included the text-preprocessing steps in the cell below. In the output cell you will notice that the first chapter of pride and prejudice is printed out. It is then preprocessed using the `preprocess_text` function and printed out again.\n",
    "\n",
    "#### **ðŸ’» 1.1. Implement the `preprocess_text` function according to the following description:**\n",
    "\n",
    "1. Remove empty characters using `.strip()`\n",
    "2. Make all text lowercase\n",
    "3. Remove all special characters by using the regex pattern.\n",
    "   - *HINT: Here you can make use of Python's [re.sub](https://www.geeksforgeeks.org/python/re-sub-python-regex/) function. Basically, we want to **keep** all characters that are not alphanumeric, I.E. lower- and uppercase A-Z, 0-9.*\n",
    "4. Split the text by newlines so we can remove chapter headlines\n",
    "   -  *HINT: `\\n` is the character used for newlines*\n",
    "5. Remove chapter headlines\n",
    "6. Join the document again, which was previously split by newlines\n",
    "7. Replace `\\n` with whitespace and double-whitespaces with single whitespaces\n",
    "\n",
    "#### **1.2. Even if implemented correctly, this preprocessing is not perfect. Do you notice any issues in the text? Show examples of words that will may be problematic:**\n",
    "\n",
    "\n",
    "*HINT: What happens to *good-humoured*? What happens to *three-and-twenty*? What happens to *Mr.* and *Mrs.*, and how will this later be handled when we split the sentences?*\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **ðŸ’» 1.3 Apply the preprocessing to the texts for today: \"pride_and_prejudice.txt\" and the \"origin_of_species.txt\"**\n",
    "\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97712834817df01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:13.183507Z",
     "start_time": "2024-10-28T08:31:13.168509Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"TODO: MISSING DOCSTRING\n",
    "\n",
    "    Args:\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3f48806447a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:14.046497Z",
     "start_time": "2024-10-28T08:31:14.032991Z"
    }
   },
   "outputs": [],
   "source": [
    "print(preprocess_text(\"good-humoured\"))\n",
    "print(preprocess_text(\"Mr.\"))\n",
    "print(preprocess_text(\"I am, but\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88535013892b3330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:14.284682Z",
     "start_time": "2024-10-28T08:31:14.215115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing of Pride and Prejudice and Origin of species here\n",
    "\n",
    "with open(\"data/pride_and_prejudice.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    pride_n_pred = file.read()\n",
    "    pride_n_pred_preproc = preprocess_text(pride_n_pred)\n",
    "\n",
    "with open(\"data/origin_of_species.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    orig_of_spec = file.read()\n",
    "    orig_of_spec_preproc = preprocess_text(orig_of_spec)\n",
    "\n",
    "print(f\"Pride and prejudice preprocessed: {pride_n_pred_preproc}\")\n",
    "print(f\"Origin of species preprocessed: {orig_of_spec_preproc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f818c01",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 2 - Creating N-grams\n",
    "\n",
    "Now that we've preprocessed our text, we can start to use it to create N-grams, and to calculate next-word probabilities.\n",
    "\n",
    "#### **2.1 How do can N-grams encode the context of a text? This is especailly when compared to methods such as just vectorizing single words?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **ðŸ’» 2.2 Implement the below function `tokenize_and_pad`. It should:**\n",
    "\n",
    "1. Split the corpus into sentences *Using a full stop as the delimiter*\n",
    "2. Calculate the length of the padding needed\n",
    "    - Think about how many words are needed to create the first N-gram **of length N**? *Hint: The first actual word is included*\n",
    "3. Create the padding for the front and end using for example `\" \".join(list_of_start_chars)`\n",
    "    - In Python you can create a list of length pad_len using `[el] * pad_len`\n",
    "4. Pad each corpus sentence ensuring that there is a space between the padding and sentence\n",
    "    - Use the .strip() method on each sentence to remove empty characters\n",
    "Split the padded sentences into words and return a list of lists containing the split sentences\n",
    "\n",
    "\n",
    "#### **ðŸ’» 2.3 Complete the `create_n_grams` function to create N-grams with length of the parameter `N` based on the tokinzed sentences. You can use the following approach:**\n",
    "\n",
    "1. Create and empty list for the n_grams\n",
    "2. Loop across all the tokenized sentences\n",
    "3. Now loop across the range of applicable starting indexes for the given sentence\n",
    "    - If N = 2, what's the length of the sentences list and how many N_grams can we create? Generalise this.\n",
    "4. Create the current n_gram by using `\" \".join`\n",
    "5. Return the n_grams list\n",
    "\n",
    "\n",
    "#### **2.4: Test the implementations of both above functions by running the cell two steps below. Does it produce the expected output? Could anything be improved?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e289604555b27e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:14.428264Z",
     "start_time": "2024-10-28T08:31:14.415266Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_pad(corpus, N):\n",
    "    \"\"\"\n",
    "    Split corpus into a list of lists so each outer list is a sentence in the corpus\n",
    "    And each inner list is the words contained in that sentence\n",
    "    With N <s> tokens added to the front of those sentenes, and N </s> tokens added at the end of each sentence\n",
    "\n",
    "    Args:\n",
    "        corpus (str): One long string of the whole corpus to tokenize\n",
    "        N (int): Length of the N-grams to generate\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: List of lists of strings as defined in the description\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return tokenized_corpus_sentences\n",
    "\n",
    "\n",
    "def create_n_grams(tokenized_sentences, N):\n",
    "    \"\"\"Take a list of lists of tokenized sentences and split into N-grams and return a list of N-grams.\n",
    "\n",
    "    Args:\n",
    "        tokenized_sentences (list[list[str]]): List of lists, each item in the outer list is a sentence, each item in the inner lists is...\n",
    "        ... either a word or a padding token of that sentence.\n",
    "        N (int): Length of the N-grams to generate\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list strings where each list item is an N-gram, either with or without padding.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return n_grams\n",
    "\n",
    "def create_n_grams_from_corpus(corpus, N):\n",
    "    \"\"\"Convenience function to call instead of two above functions one after the other\n",
    "\n",
    "    Args:\n",
    "        corpus (str): String with whole corpus to create N-grams from\n",
    "        N (int): Length of the N-grams to generate\n",
    "    Returns:\n",
    "        list[str]: List of all N-grams. <s> or </s> tokens added to ensure all are of length N\n",
    "    \"\"\"\n",
    "    return create_n_grams(tokenize_and_pad(corpus, N), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792e4ef5c318a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:14.475797Z",
     "start_time": "2024-10-28T08:31:14.450796Z"
    }
   },
   "outputs": [],
   "source": [
    "# chosen_text = orig_of_spec_preproc\n",
    "chosen_text = pride_n_pred_preproc\n",
    "\n",
    "# Tokenize and pad the text\n",
    "N = 3\n",
    "chosen_text_tokenize = tokenize_and_pad(chosen_text, N=N)\n",
    "print(chosen_text_tokenize[:5])\n",
    "\n",
    "# Create n-grams\n",
    "chosen_text_n_grams = create_n_grams(chosen_text_tokenize, N=N)\n",
    "print(chosen_text_n_grams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069eb31",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 3 - Calculating word probabilites\n",
    "\n",
    "Now that we have created our N-grams, we can use our texts as a training set to naively calculate next-word probabilities based solely on examples.\n",
    "\n",
    "\n",
    "#### ðŸ’» **3.1 Read the function `n_grams_to_prob_map` which takes a list of n_grams as returned by the function above and create a dictionary which maps the probabilities of words occurring after a given context. In particular, try to find out how it actually calculates the probability of a word or N-gram appearing in a certain context**\n",
    "\n",
    " <!-- **ðŸ’» 3.1 Implement the function `n_grams_to_prob_map` which takes a list of n_grams as returned by the function above and create a dictionary which maps the probabilities of words occurring after a given context. You can use the following approach:** -->\n",
    "\n",
    "<!-- 1. If you are unfamiliar with the defaultdict class in Python, read this article before you start this task https://www.geeksforgeeks.org/defaultdict-in-python/\n",
    "2. If you are unfamiliar with the lambda keyword in Python, read this article before you start this task https://www.geeksforgeeks.org/python-lambda-anonymous-functions-filter-map-reduce/?ref=header_outind\n",
    "3. Create a default dict, name it `contexts`, with a default value that is a default dict whose default value is 0\n",
    "    - In essence: defaultdict(defaultdict(0))\n",
    "    - This will be used to count how often each target word occurs after a certain context\n",
    "4. Loop through every n-gram and split it\n",
    "5. Create the context by using `\" \".join` on all but the last token in the n-gram. The last token is the target\n",
    "6. Increment (means adding 1 to a value) the counter for the target value for the given context\n",
    "7. Create a new dictionary `cond_prob`\n",
    "8. Loop across the keys of `contexts`\n",
    "9. Create a list of each target in the current context by wrapping the .keys() call in a list()\n",
    "10. Count the number of occurrences of each target in the current context and put them in a numpy array\n",
    "11. Calculate the probabilities of each target by normalising the occurrence count \n",
    "    - Ensure it sums to 1\n",
    "12.  Assign a tuple of the targets and their probabilities to the entry in the `cond_prob` function\n",
    "    - The order of the tuple is important for later, so (context_targets, targets_probs) -->\n",
    "\n",
    "#### **3.2 Inspect the plot of the targets does the \"top contender\" make sense?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **3.3 Change the chosen_text to be the *Origin of Species* without changing anything else. Look at the plot now, why do you think we started with *Pride and Prejudice?***\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **3.4. Vary the N-gram size, `N`. Inspect the first 20 N-grams created now. What might change with the conditional probabilities, and what problems might arise from this?.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8a8b43a597e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:14.696107Z",
     "start_time": "2024-10-28T08:31:14.688108Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_grams_to_prob_map(n_grams):\n",
    "    \"\"\"Given a text of n-grams, create a mapping so each n-gram can be considered a context with different targets that appear with a given probability\n",
    "\n",
    "    NOTE: If only unigrams (N=1) added. No context is present for any single word, thus no word conditional probabilities can be inferred\n",
    "\n",
    "    Args:\n",
    "        list[str]: List of all N-grams. <s> or </s> tokens added to ensure all are of length N\n",
    "\n",
    "    Returns:\n",
    "        dict[str: tuple(list[str], np.ndarray[float])]: dict where the keys are contexts, the values are tuples of targets and probabilities...\n",
    "        ... where the targets[i] appears with probabilities[i] probability in the given context\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generally, using lambdas is seen as bad practice by the Python community. We make an exception for default dicts\n",
    "    defaultdict_function = lambda: defaultdict(lambda: 0)\n",
    "    \n",
    "    # Create a sort of nested default dict...\n",
    "    # ... This is so that each context is initialized as a defaultdict where each word in that context\n",
    "    # ... starts as probability 0 to be in that context\n",
    "    contexts = defaultdict(defaultdict_function)\n",
    "\n",
    "    # Get the size of the N-grams\n",
    "    N = len(n_grams[0].split(\" \"))\n",
    "\n",
    "    for n_gram in n_grams:\n",
    "        # Split N-grams (which are strings) on whitespace\n",
    "        n_gram_split = n_gram.split(\" \")\n",
    "\n",
    "        # Get the context as the N-1 preceding words before a given word\n",
    "        context = \" \".join(n_gram_split[:N-1])\n",
    "\n",
    "        # The target is then the last word after the given context\n",
    "        target = n_gram_split[N-1]\n",
    "\n",
    "        # Increment the amount of times we have seen that specific target after that specific context by 1\n",
    "        contexts[context][target] += 1\n",
    "\n",
    "    cond_prob = {}\n",
    "    for context in contexts.keys():\n",
    "        # Get all words that appear AFTER the current context\n",
    "        context_targets = list(contexts[context].keys())\n",
    "\n",
    "        # targets_count is an array of how many times we have seen every target that appears after given context\n",
    "        targets_count = np.array([contexts[context][target] for target in context_targets])\n",
    "\n",
    "        # Sum how many appearances there are in total\n",
    "        context_sum = np.sum(targets_count)\n",
    "\n",
    "        # Get the probability by dividing each count by the total number of appearances\n",
    "        targets_probs = targets_count / context_sum\n",
    "        cond_prob[context] = (context_targets, targets_probs)\n",
    "    return cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63d602ce669b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:17.479003Z",
     "start_time": "2024-10-28T08:31:14.753624Z"
    }
   },
   "outputs": [],
   "source": [
    "chosen_text_cond_prob = n_grams_to_prob_map(chosen_text_n_grams)\n",
    "\n",
    "chosen_n_gram = 60 # The phrase \"he is considered\" for pride and prejudice\n",
    "context_test = chosen_text_n_grams[chosen_n_gram].split(\" \")[:-1] # Get the last word in this phrase\n",
    "context_test = \" \".join(context_test)\n",
    "\n",
    "# Index out probabilities with this given context\n",
    "targets, probs = chosen_text_cond_prob[context_test]\n",
    "\n",
    "# Sort the targets and probabilities for convenience\n",
    "sorted_targets, sorted_probs = zip(*sorted(zip(targets, probs), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Plot the targets\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_targets, sorted_probs, color='skyblue')\n",
    "plt.xlabel(\"Target Words\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Conditional Probability Distribution for Context: '{context_test}'\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()  # Adjust layout for readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9b9f9d2251179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:35:06.058945Z",
     "start_time": "2024-10-28T10:35:02.462573Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3.4: Inspecting first 20 N-grams\n",
    "N=4\n",
    "orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "orig_of_spec_n_grams = create_n_grams(orig_of_spec_tokenize, N=N)\n",
    "orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)\n",
    "print(orig_of_spec_n_grams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e9e1a",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 4 - Generating text with conditional probabilities\n",
    "\n",
    "In the previous exercise we saw how to tokenize a corpus such that it is ready to be split into n-grams. We then saw how to make n-grams and create a conditional probability based on these.\n",
    "\n",
    "The question now is, how can we generate a text using this conditional probability. A way of doing this is to sample from a conditional probability distribution based on our obtained N-grams. In essence, we can give a seed to our conditional probability (also called a context), and then we need to generate a word from our conditional probability by sampling from it.\n",
    "\n",
    "In the code below we will define a function that allows us to generate a sentence based on a provided conditional distribution. In the cell we create such a conditional distribution and generate 5 sentences using the same text seed. \n",
    "\n",
    "#### **4.1. ðŸ’» Implement the `generate_text` function. Given a seed start text and some context probabilities (as created by the previous exercises), it should generate additional text.**\n",
    "\n",
    "\n",
    "You can use  following as help to implement it:\n",
    "\n",
    "1. Create a variable which will be the output string and assign the text_seed to it\n",
    "2. Split the text_seed into words and check its length, it should be N-1\n",
    "    - If the text seed is too short append the start character to it\n",
    "    - If the text seed is too long change it to the last N-1 words\n",
    "3. Create a variable that holds the current context (text_seed right now)\n",
    "4. Loop across the number of words we wish to generate\n",
    "   - If the current context is not in the `cond_prob` dictionary, return the generated sentence\n",
    "   - If it is, sample a target word from the context according to its probability distribution and update the context for the next iteration\n",
    "\n",
    "#### **4.2. Answer the following questions about the text generation you have just implemented:**\n",
    "\n",
    "1. Why is it that even though we use the same text seed, the generated sentences changes?\n",
    "2. What happens as you increase the N-gram size as shown in the cell below? Does this makes sense - and if so, why?\n",
    "3. Is it more optimal to have smaller or larger N-gram size? Try to experiment with generated sentences as N goes from 2->7.\n",
    "4. What would it mean to set the N-gram size to one? What would you expect the generated text to look like?\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90afe4279bf81ede",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:18.542201Z",
     "start_time": "2024-10-28T08:31:18.530163Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(cond_prob, text_seed, N, num_words=25):\n",
    "    \"\"\"TODO: MISSING DOCSTRING!\n",
    "\n",
    "    Args:\n",
    "        cond_prob (_type_): _description_\n",
    "        text_seed (_type_): _description_\n",
    "        N (_type_): _description_\n",
    "        num_words (int, optional): _description_. Defaults to 25.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    pass    \n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2339b75e29fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:18.918672Z",
     "start_time": "2024-10-28T08:31:18.861637Z"
    }
   },
   "outputs": [],
   "source": [
    "N=2 # N-grams of length 2\n",
    "orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "orig_of_spec_n_grams = create_n_grams(orig_of_spec_tokenize, N=N)\n",
    "orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)\n",
    "\n",
    "text_seed = \"he is a\"\n",
    "for i in tqdm(range(5)):\n",
    "    generated_text = generate_text(cond_prob=orig_of_spec_cond_prob, text_seed=text_seed, N=N) \n",
    "    print(generated_text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36a682",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 5 - Varying N-gram length for text generation\n",
    "\n",
    "It can be interesting to examine how the text generation can vary given with the length of our N-grams. Here, we will do so.\n",
    "\n",
    "#### **5.1. ðŸ’» Write code to create N-grams and conditional probability maps using Pride and Prejudice as the corpus for sizes of N-grams varying from $N = 1,2, \\dots,5$. Answer the following questions:**\n",
    "\n",
    "1. What happens with the total number of contexts as $N$ increases? Why does it increase like this?\n",
    "2. Why is the number of contexts in the case of $N=1$ empty? (It should be if you implemented `n_grams_to_prob_map` and `create_n_grams` correctly!)\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59edaf21e94dfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:24:34.635728Z",
     "start_time": "2024-10-28T10:24:23.999831Z"
    }
   },
   "outputs": [],
   "source": [
    "for N in tqdm(range(1, 5+1)):\n",
    "\n",
    "    print(f\"Maximum N-gram length: {N}: Number of contexts: {num_contexts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01abd020",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 2: Fasttext for 'Ham og Spam'\n",
    "\n",
    "In the following exercises we will be looking at classifying text messages as \"Ham\" or \"Spam\" by using Fasttext models. \n",
    "\n",
    "- First, we will format our data to comply with the Fasttext library,\n",
    "- Then, we will implement our own Pytorch model to run on the data\n",
    "- Finally, we will use the much faster Fasttext library to achieve the same results.\n",
    "\n",
    "**There isn't much coding in this section (we removed a lot of the exercises), since it would probably take too long for you to implement. If you want a version with no coding answers, you can open the extra file \"ex_week_8_more_coding.ipynb\" instead**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f29aa0",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 6 - Preparing spam or ham data\n",
    "\n",
    "In the following cells we use the pandas library to load our text delimited file which has the labels in the first column and the text messages in the second. Fasttext expects a file where each line has the signature  `__label__{label} text` (where `__label__` is a token, i.e. something the FastText library reads as a keyword).\n",
    "\n",
    "#### **ðŸ’» 6.1 Create a function that given a dataframe containing a column of texts and a column of labels, creates a .txt file where each line starts with `__label__x y` where `x` is the label and `y` is the text.**\n",
    "\n",
    "You can use the following as a guide to implement it\n",
    "\n",
    "1. Iterate across the labels and text and write each line to a string in the correct format\n",
    "2. Use a `with open(path_to_doc)` clause to write the text to a document\n",
    "3. Include a print statement tha prints the distribution of labels in the test which is only executed if `verbose=True`\n",
    "\n",
    "\n",
    "\n",
    "#### **ðŸ’» 6.2 Use the `pandas.read_csv` function to read the `SMS_train.txt` file. Answer the following questions, either by Google or inspecting the dataset:**\n",
    "\n",
    "- How many text messages are in the training set?\n",
    "- What's the distribution of labels and how will that impact our training of a classifier? \n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1024b9656df3a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:30.670376Z",
     "start_time": "2024-10-28T08:31:30.656338Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_fasttext_format_txt(data_frame, path_to_doc, v=True):\n",
    "    \"\"\"TODO: MISSING DOCSTRING!\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): Data frame with train- or test-data. \n",
    "            feature and label column should be named \"1\" and \"0\" respectively\n",
    "        path_to_doc (str): Where to write the output to\n",
    "        v (bool, optional): Whether to output additional information about dataset. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        texts (list[str]): list of input texts to the eventual model \n",
    "        labels ([list[str]]) list of \"ham\" or \"spam\"\n",
    "    \"\"\"\n",
    "\n",
    "    pass    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004909b3d1703dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:30.920418Z",
     "start_time": "2024-10-28T08:31:30.800897Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(\"data\", \"SMS_train.txt\"))\n",
    "test_data = pd.read_csv('./data/SMS_test.txt', delimiter=',', encoding=\"utf-8\")\n",
    "\n",
    "train_texts, train_labels = create_fasttext_format_txt(data_frame=train_data, path_to_doc='data/train_data.txt')\n",
    "test_texts, test_labels = create_fasttext_format_txt(data_frame=test_data, path_to_doc='data/test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560fa2dcd23c983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:31:31.095540Z",
     "start_time": "2024-10-28T08:31:31.081531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display some mixed exmaples of the train data\n",
    "display(train_data)\n",
    "\n",
    "# Display some examples of the test data where the label is \"spam\"\n",
    "display(test_data[test_data['0'] == \"spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a23288a7d9d97e",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 7 - Our own fasttext\n",
    "\n",
    "As you should know by now there are libraries for most things in Python and Fasttext is no Exception. Nonetheless, it is valuable to build the architecture from \"scratch\" to truly understand what is going on. \n",
    "We will use Pytorch, which means we have to create our own dataloader. Since we are working with text data that is embedding through a learned embedding, we will include the embedding part of the network in the Dataloader which is probably not best practice, but done for coding simplicity. \n",
    "\n",
    "The embedding module takes as input the indices of each word in a given vocabulary and outputs a vector of the specified dimension as output. This way the gradient can be backpropagated to the embedding by changing the embedding of a corresponding index based on the error. Ask a TA if you want a more in-depth explanation.\n",
    "\n",
    "Typical hyperparameters for Fasttext\n",
    "- Embedding dimension: 300\n",
    "- Batch size: low (1 - you'll help figure this out)\n",
    "- N_grams: 3-6\n",
    "- Learning rate: the classic $10^{-3}$ to $10^{-5}$\n",
    "\n",
    "#### **ðŸ’» 7.1: Implement the functions `process_line`, `create_word_grams_from_sentence`, and `create_char_grams_from_sentence`**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- **If you do not want to implement this by yourself, simply copy the dataloader from the solution and move on**\n",
    "\n",
    "The Dataloader has to accomplish the following:\n",
    "- Loading the txt file of the format created above\n",
    "- Processing each line of text\n",
    "    - Use the parts of `preprocess_text` that apply to a single sentence\n",
    "    - Create the word n-grams or character n-gram of each text using the two functions defined in the cells before the dataloader \n",
    "- Build the vocabulary, we must be able to pass the vocabulary from the train loader to the test loader\n",
    "- Count the distribution of classes\n",
    "- Create a new `nn.Embedding` module unless one is passed to the dataloader. For testing we need to use the same embedding as we did for training and thus we need to be able to re-use it in the Dataloader.\n",
    "\n",
    "**1. $\\star$ Implement the following functions to be used for processing a line of text in the Fasttext format and creating word and character grams**  -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7b7838c7f6584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:18:19.048964Z",
     "start_time": "2024-10-28T09:18:19.035451Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    \"\"\"TODO: MISSING TYPEHINTS\n",
    "    Given a line assuming the '__label__{label} text' format, split it into the label and text, strip the text, make it lowercase\n",
    "    remove special characters and remove newlines and double spaces\n",
    "    Args:\n",
    "        line (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    label, text = line.split(maxsplit=1)\n",
    "    label = label.replace('__label__', '').strip()\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.?! \\n]+\", \"\", text)\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    return label, text\n",
    "\n",
    "with open('data/train_data.txt', \"r\") as f:\n",
    "    for line in f.readlines()[:10]:\n",
    "        print(process_line(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb8fff5fa360ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:25:11.049040Z",
     "start_time": "2024-10-28T09:25:11.037038Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_word_grams_from_sentence(sentence, N):\n",
    "    \"\"\"TODO: MISSING TYPEHINTS\n",
    "    Given a sentence and int N, pad the sentence with the start and end character and return a list of word N-grams\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sentence (_type_): _description_\n",
    "        N (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return sentence_n_grams\n",
    "\n",
    "example_word_grams = create_word_grams_from_sentence(\"a simpel simple sentence\", 2,)\n",
    "print(example_word_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88c7d396f02dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:25:21.291347Z",
     "start_time": "2024-10-28T09:25:21.281351Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_char_grams_from_sentence(sentence, min_n, max_n):\n",
    "    \"\"\"TODO: MISSING TYPEHINTS\n",
    "    Given a sentence and the range of character n-grams to produce, return a list of the character grams\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sentence (_type_): _description_\n",
    "        min_n (_type_): _description_\n",
    "        max_n (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return all_char_grams\n",
    "\n",
    "example_character_grams = create_char_grams_from_sentence(\"a simpel simple sentence\", 2, 3)\n",
    "print(example_character_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09d3eac76b3adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:35:49.751098Z",
     "start_time": "2024-10-28T09:35:49.730038Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    \"\"\"Given a list of texts, count the number of unique words and map each to an index. Add an unknown at the end (for safety)\"\"\"\n",
    "    # Look up the Counter class in python for getting the frequencies of each object\n",
    "    # This will be used to count the number of unique words\n",
    "    pass\n",
    "    return vocab\n",
    "build_vocab([example_word_grams for _ in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e839fe7fdb9e46",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Here we implement a subclass of torch.utils.data.Dataset to make loading and processing files much easier.\n",
    "\n",
    "The real kicker, apart from encapsulation, is that we can implement our own `__getitem__` and `__len__` functions. `__getitem__` is a [Python 'magic method'](https://realpython.com/python-magic-methods/) that is called whenever we iterate over an iterable, for example in a for-loop.\n",
    "\n",
    "\n",
    "*The dataset is of a type [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html), usually used when we need a bit more functionality than just yielding items from a list or array*\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e50d4f681d5609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:58:10.341278Z",
     "start_time": "2024-10-28T09:58:10.311275Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, embed_dim=300, embedding=None, vocab=None, N_gram=3, minn=0, maxn=0, ):\n",
    "        self.data = []\n",
    "        self.labels = set()\n",
    "        self.class_counts_dict = defaultdict(lambda: 0)\n",
    "        self.N_gram = N_gram\n",
    "        self.minn = minn\n",
    "        self.maxn = maxn\n",
    "        print(f\"Using {'word' if self.minn == 0 or self.maxn == 0 else 'char'} gram model\")\n",
    "        self.process_file(file_path=file_path)\n",
    "        \n",
    "        # Build vocabulary and label-to-index mappings\n",
    "        self.vocab: dict = build_vocab([text for text, label in self.data]) if vocab is None else vocab\n",
    "        self.vocab_size: int = len(self.vocab)\n",
    "        self.label_to_idx: dict = {label: idx for idx, label in enumerate(sorted(self.labels)[::-1])}\n",
    "        self.num_classes: int = len(self.label_to_idx)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embed_dim) if embedding is None else embedding\n",
    "        sorted_labels: list = sorted(self.class_counts_dict.keys(), key=lambda label: self.label_to_idx[label])\n",
    "        # Sort class_counts based on the sorted order of labels\n",
    "        self.class_counts: torch.tensor = torch.tensor([self.class_counts_dict[label] for label in sorted_labels])\n",
    "    \n",
    "    def process_file(self, file_path):\n",
    "        # Load and process data\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                label, text = process_line(line)\n",
    "                \n",
    "                if not text.strip():\n",
    "                    print(line.replace('\\n', '') + f\" = {text} was skipped\")\n",
    "                    continue\n",
    "                \n",
    "                if self.minn == 0 or self.maxn == 0:\n",
    "                    text = create_word_grams_from_sentence(text, N=self.N_gram)\n",
    "                else:\n",
    "                    text = create_char_grams_from_sentence(text, min_n=self.minn, max_n=self.maxn)\n",
    "                \n",
    "                self.labels.add(label)\n",
    "                self.data.append((text, label))\n",
    "                self.class_counts_dict[label] += 1\n",
    "\n",
    "    def text_embedded(self, text):\n",
    "        \"\"\" Convert each token to its corresponding index using the vocabulary.get() setting the default to the unknown token.\n",
    "        Then use the embedding module to embed the indices and take the mean across dim=0.\"\"\"\n",
    "        text_indices = torch.tensor([self.vocab.get(word, self.vocab['<UNK>']) for word in text]).int()\n",
    "        embedded = self.embedding(text_indices).mean(dim=0)\n",
    "        if torch.isnan(embedded).any():\n",
    "            print(f\"Embedding contains nans!\")\n",
    "            print(text)\n",
    "        return embedded\n",
    "\n",
    "    def label_to_tensor(self, label):\n",
    "        \"\"\"Convert the label to its corresponding index, cast it to a tensor and call the .long() method on the tensor to ensure the correct dtype\"\"\"\n",
    "        return torch.tensor(self.label_to_idx[label]).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of datapoints in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the text and label at the given index by converting the label to a tensor and the embedding the text.\"\"\"\n",
    "        text, label = self.data[idx]\n",
    "        return self.text_embedded(text), self.label_to_tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c968432",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "We then implement a single layer that maps from embedding dimension $\\rightarrow$ output dimension\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3483dad9325d43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:41:18.570493Z",
     "start_time": "2024-10-28T09:41:18.560490Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e620e",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "And we then get a function that given a number appearences for each class can output the inverse class frequency, to give us an idea of the relative proportion of each class in the dataset.\n",
    "\n",
    "The inverse class frequency for a given class is $ICF(c_1) = \\frac{\\sum_i^{N}count(c_i)}{count(c_1)}$, where $N$ is the number of classes and $count(x)$ counts the number of occurrences of a given class in the data\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81d95f5a3c043e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:49:20.621061Z",
     "start_time": "2024-10-28T09:49:20.607019Z"
    }
   },
   "outputs": [],
   "source": [
    "def ICF(class_counts):\n",
    "    return (torch.sum(class_counts) / class_counts).float()\n",
    "\n",
    "ICF(torch.tensor((3, 1)))   # Inputting (3, 1) should yield (4/3, 4/1) = (1.3333333, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d10c03",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 8 - Training our own FastText\n",
    "\n",
    "\n",
    "Finally, we train the network in the training loop below. Here you can test whether different configurations of batch_size, embedding_dim, and so on, makes a difference.\n",
    "\n",
    "#### **8.1 We calculate both sensetivity and specificity. Look at their calculations in the code, and explain what they measure**\n",
    "\n",
    "It calculates the proportion of each class which is classified correctly. In other words, in our case sensitivity is how well spam is detected and specificity how well ham is detected.\n",
    "\n",
    "#### **8.2 Explain briefly what each of the following parameters means:**\n",
    "\n",
    "- `N_gram`\n",
    "- `minn`\n",
    "- `maxn`\n",
    "- `lr`\n",
    "- `embed_dim`\n",
    "- `batch_size`\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **8.3 Look at the confusion matrix, does the result seem good?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e921943c18c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:44:30.633249Z",
     "start_time": "2024-10-28T09:44:30.364364Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "N_gram = 2\n",
    "minn = 2\n",
    "maxn = 3\n",
    "# If we specify BOTH minn and maxn, we use a character gram model\n",
    "dataset_train = TextDataset('data/train_data.txt', embed_dim=embed_dim, N_gram=N_gram, minn=minn, maxn=maxn)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "class_weights = ICF(dataset_train.class_counts)\n",
    "print(\"Class_weights:\", class_weights)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fdc49dc920fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:48:33.947570Z",
     "start_time": "2024-10-28T09:44:31.383100Z"
    }
   },
   "outputs": [],
   "source": [
    "model_save_dir = \"models_char\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model = TextClassifier(embed_dim=embed_dim, num_classes=dataset_train.num_classes)  # Adjust vocab_size and num_classes as needed\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "metrics = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    accuracy_train = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for texts_embedded, labels in tqdm(dataloader_train, desc=f\"Epoch {epoch}\"):\n",
    "        outputs = model(texts_embedded)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = outputs.argmax(1)\n",
    "        accuracy_train += torch.sum(preds == labels)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_dir, f\"{epoch}.pth\"))\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}, '\n",
    "          f'Accuracy: {accuracy_train/len(all_labels):.4f}, '\n",
    "          f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "    metrics.append([loss.item(), accuracy_train/len(all_labels), sensitivity, specificity])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e9393841559a6",
   "metadata": {},
   "source": [
    "The following cell display the latest confusion matrix. Due to the sorting we do (in the solution Dataloader), spam is the positive class and ham the negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e4027399d4ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:49:04.773738Z",
     "start_time": "2024-10-28T09:49:04.596660Z"
    }
   },
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset_train.labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7445d346fcd83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:43:18.537755Z",
     "start_time": "2024-10-28T09:43:18.527199Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    metrics = np.array(metrics)\n",
    "    metric_names = ['Loss', 'Accuracy', 'Sensitivity', 'Specificity']\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    # Create plots\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i in range(len(metric_names)):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.plot(range(1, len(metrics[:, i])+1), metrics[:, i], marker='o', label=metric_names[i], color=colors[i])\n",
    "        plt.title(f'{metric_names[i]} over Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric_names[i])\n",
    "        plt.xticks(range(1, len(metrics[:, i]) +1))\n",
    "        if i == 0:\n",
    "            plt.ylim(bottom=0)  # Only for Loss, no upper limit\n",
    "        else:\n",
    "            plt.ylim(0, 1)  # Set y-limits for accuracy, sensitivity, specificity\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc60d24138eb5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:49:35.770896Z",
     "start_time": "2024-10-28T09:49:34.472686Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3febab4a12a408",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "During training, we save the network after each epoch, here we just load the network after a number of epochs and run it on the test data.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7782b6de186da2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:43:27.274187Z",
     "start_time": "2024-10-28T09:43:27.250190Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_load = 9\n",
    "model.load_state_dict(torch.load(os.path.join(model_save_dir, f\"{epoch_load}.pth\"), weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f48187eb35cca",
   "metadata": {},
   "source": [
    "**7. Test the network using the loop below:**\n",
    "- How well does it perform, would you use this for spam detection in a real application?\n",
    "- Can you improve it?\n",
    "- The comparison between a word and character model is especially interesting if you consider what the data consists of, why?\n",
    "The data contains a LOT of spelling mistakes and a word gram model will see two words that are the same but mispelled as two unique words, whereas the character model will have many tokens that end up the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b1962bd1df761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:19:19.936894Z",
     "start_time": "2024-10-28T10:19:19.055476Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_test = TextDataset('data/test_data.txt', vocab=dataset_train.vocab, embedding=dataset_train.embedding, minn=minn, maxn=maxn)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "accuracy_test = 0\n",
    "loss_test = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "for texts_embedded, labels in dataloader_test:\n",
    "    outputs = model(texts_embedded)\n",
    "    loss_test += criterion(outputs, labels) \n",
    "    \n",
    "    preds = outputs.argmax(1)\n",
    "    accuracy_test += torch.sum(preds == labels)\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tp, fn, fp, tn = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Test loss: {loss_test.item():.4f}, Test accuracy: {accuracy_test / len(all_labels):.4f} \"\n",
    "      f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset_test.labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f379a",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 3: Doing it again with the FastText library.\n",
    "\n",
    "All that we did above is a good example, but an arguably better implementation of fasttext already exists, the package by the same name.\n",
    "\n",
    "In the following cells, we will train a model using this package and examine its output.\n",
    "\n",
    "There are a number of parameters that can be passed to the FastText `train_supervised` function, but we will just concern ourselves with a couples of them.\n",
    "\n",
    "*The `input` parameter requires a text file as an input containing two columns. The first column must be the classification label and the second must be the text. The format we implemented above*\n",
    "*The `verbose` parameter just allows us to enable or disable training information. Here we enable it.*\n",
    "\n",
    "\n",
    "**A lot of the code here, might help you when doing assignment 3, where you also have to work with the fasttext package...**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We deferred the import to here so people wouldn't get stuck trying to install it before doing other exercises...\n",
    "import fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc0cae96cf4629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:02:25.185718Z",
     "start_time": "2024-10-28T10:02:24.379763Z"
    }
   },
   "outputs": [],
   "source": [
    "fasttext_word_model = fasttext.train_supervised(input='./data/train_data.txt', verbose=True, wordNgrams=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734e0c8",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Here we implement a function to test fasttext given a list of test texts, test labels, and our model. Essentially we just:\n",
    "\n",
    "1. Loop across the test_texts and labels and use the model to predict the label\n",
    "    - `model.predict(text)[0][0]` returns a string with the signature `__label__{label}`\n",
    "2. Keep track of the predictions and labels in two separate lists\n",
    "3. Use the same function to calculate and display the confusion matrix\n",
    "4. Calculate the specificity and sensitivity using the same formulas as we did for the other model\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c29d7e69d66b3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:20:24.313246Z",
     "start_time": "2024-10-28T10:20:24.299659Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_fasttext_model(test_texts, test_labels, fasttext_model, verbose=False):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for text, label in zip(test_texts, test_labels):\n",
    "        prediction = fasttext_model.predict(text)[0][0].replace('__label__', '')\n",
    "        predictions.append(prediction)\n",
    "        true_labels.append(label)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    labels = sorted(set(test_labels))[::-1]  # Ensure consistent label order\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=labels)\n",
    "\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    accuracy = (tp + tn) / np.sum(cm)\n",
    "\n",
    "    # Print metrics if verbose\n",
    "    if verbose:\n",
    "        # Display confusion matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "    \n",
    "        print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "        print(f\"Specificity: {specificity:.2f}\")\n",
    "        print(f'Model accuracy: {accuracy * 100:.2f} %')\n",
    "    \n",
    "    return accuracy, cm, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e63e9ecadb7d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:20:26.489576Z",
     "start_time": "2024-10-28T10:20:25.317948Z"
    }
   },
   "outputs": [],
   "source": [
    "fasttext_word_model = fasttext.train_supervised(input='./data/train_data.txt', verbose=True, wordNgrams=5)\n",
    "accuracy_word_model, cm_word_model, sens_word_model, spec_word_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_word_model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7ba4b",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 9 - Varying N-grams\n",
    "\n",
    "\n",
    "#### **9.1 What happens when you vary the N-gram size when testing? What appears to be the optimnal setting, and why do you think this is the case?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516b5da6cdf5bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:20:39.285805Z",
     "start_time": "2024-10-28T10:20:33.723011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vary the size of N-grams and check its accuracy across all\n",
    "\n",
    "metrics = []\n",
    "for N in range(1, 8):\n",
    "    fasttext_word_model = fasttext.train_supervised(input='./data/train_data.txt', verbose=True, wordNgrams=N)\n",
    "    accuracy_word_model, cm_word_model, sens_word_model, spec_word_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_word_model, verbose=False)\n",
    "    metrics.append((N, accuracy_word_model, sens_word_model, spec_word_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e29deb1bb926c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:20:50.003687Z",
     "start_time": "2024-10-28T10:20:49.702976Z"
    }
   },
   "outputs": [],
   "source": [
    "n_values, accuracies, sensitivities, specificities = zip(*metrics)\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_values, accuracies, marker='o', label=\"Accuracy\")\n",
    "plt.plot(n_values, sensitivities, marker='o', label=\"Sensitivity (Spam)\")\n",
    "plt.plot(n_values, specificities, marker='o', label=\"Specificity (Ham)\")\n",
    "\n",
    "# Labeling\n",
    "plt.title(\"Metrics vs. Word N-grams in FastText Model\")\n",
    "plt.xlabel(\"N (Word N-grams)\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.ylim(0, 1.1)  # Since metrics are between 0 and 1\n",
    "plt.xticks(n_values)  # Show only integer N values on the x-axis\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17198c87",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 10 - Character level model\n",
    "\n",
    "\n",
    "#### **10.1 In the cell below, we use a fasttext model with character-grams. Does it give better performance? Why do think that is/isn't?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d45ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:21:16.539043Z",
     "start_time": "2024-10-28T10:21:14.826977Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create char model here.\n",
    "char_gram_length_min = 1 # If set to zero, we only train word-grams\n",
    "char_gram_length_max = 3 # If set to zero, we only train word-grams\n",
    "\n",
    "fasttext_char_model = fasttext.train_supervised(\n",
    "    input='./data/train_data.txt',\n",
    "    verbose=True,\n",
    "    minn=char_gram_length_min,\n",
    "    maxn=char_gram_length_max\n",
    ")\n",
    "accuracy_char_model, cm_char_model, sens_char_model, spec_char_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_char_model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4c978",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Finally, below is a small test of different `maxn` and `minn` parameters. Where we examine the accuracy of each model with the given parameters, to find the (seemingly) optimal values.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:22:14.213981Z",
     "start_time": "2024-10-28T10:21:33.537271Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 1\n",
    "stop = 8\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(start, stop)):\n",
    "    temp_results = []\n",
    "    for j in tqdm(range(start, stop)):\n",
    "\n",
    "        fasttext_char_model = fasttext.train_supervised(\n",
    "            input='./data/train_data.txt',\n",
    "            verbose=True,\n",
    "            minn=i,\n",
    "            maxn=j,\n",
    "        )\n",
    "        accuracy_char_model, cm_char_model, sens_char_model, spec_char_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_char_model, verbose=False)\n",
    "        temp_results.append(accuracy_char_model)\n",
    "    results.append(temp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e279cb086a8da92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:22:14.546390Z",
     "start_time": "2024-10-28T10:22:14.220981Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(results, annot=True, cmap=\"viridis\", xticklabels=range(1, 8), yticklabels=range(1, 8))\n",
    "plt.xlabel(\"maxn (range parameter)\")\n",
    "plt.ylabel(\"minn (range parameter)\")\n",
    "plt.title(\"Heatmap of Accuracy for Different `minn` and `maxn` Values\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signals-and-data-autumn-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
