{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "# Week 7 - Text representations\n",
    "\n",
    "This week'll focus on basic text representations using the **bag-of-words**-model (BoW). We will use this to, among other things perform **Latent semantic analysis** (LSA) using **Singular value decomposition** (SVD), and later, we'll do **sentiment analysis** of texts.\n",
    "\n",
    "\n",
    "Many of the exercises may not seem all that difficult mathematically speaking, but are quite challenging from a code standpoint. As always, we encourage you to work with them, but not be afraid to skip exercises if they are too hard to code, or if you spend time purely on implementation work. **You will not be asked to write code for the exam.**. Even so, coding these exercises may be extremely useful for understanding the concepts that are behind this part of natural langauge processing.\n",
    "\n",
    "---\n",
    "\n",
    "**At the end of this week, you should be able to:**\n",
    "\n",
    "- Understand what a bag-of-words representation is, and why it is important for how we process texts\n",
    "- Explain some up- and downsides of the bag-of-words-representation\n",
    "- Understand what the term-by-term and document-by-document matrices mean, and what their individual values mean.\n",
    "- Understand how LSA is perforemed and what role SVD plays here\n",
    "- Understand how LSA can be used to visualize similarities and/or differences in both documents and terms\n",
    "- Understand what sentiment analysis is\n",
    "- Understand what is required to perform sentiment analysis\n",
    "- Understand how sentiment analysis is implemented\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Creating a BoW representation of a corpus is fairly straightforward:\n",
    "\n",
    "1. Find the number of unique words across all documents in the corpus\n",
    "2. Initialize a zero-vector for each document in the corpus, its length should be equal to the number of unique words across all documents\n",
    "3. For each document in the corpus, count the occurances of each unique word\n",
    "4. Set these number of occurances in that document, as the value for the corresponding word in the corresponding vector\n",
    "5. For $m$ documents and $n$ unique words, this should leave you with an $n \\times m$ matrix we can call $A$. the number $a_{i,j}$ will then be how many times the i'th word appears in the j'th document\n",
    "6. Optionally now, you can remove rows that correspond to stop words (this is often based on a known list of stop words)\n",
    "\n",
    "This can then be expanded to include **bigrams**, **trigrams**, and in general, **n-grams**. This is simply done by increasing the number of unique words to include all unique bigrams, trigrams, etc. The result in any case, should be something akin to what we see below (though transposed in our case).\n",
    "\n",
    "<img src=\"images/stuff.png\"\n",
    "        width = 400\n",
    "        height = 300\n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "---\n",
    "\n",
    "LSA uses the BoW to attempt to analyze relationships between sets of documents and their words. It has one simple assumption:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "  <em>\"Words that mean similar things, appear in similar contexts\"</em>\n",
    "</div>\n",
    "\n",
    "In short, LSA can tell us two things in particular:\n",
    "\n",
    "1. Documents that share a lot of words, must be closely related in subject\n",
    "2. Words that appear in the same type of documents must be closely related in meaning\n",
    "\n",
    "We perform LSA kinda like we do Principal Component Analysis (PCA), in this case, we use SVD. For the specific details of SVD, refer to the slides for week 7, page ~21.\n",
    "\n",
    "For the purposes for implementation, we are interested in matrices in particular, which we get from our $n \\times m$ BoW matrix $A$:\n",
    "\n",
    "1. The term-by-term matrix, a $n \\times n$ matrix, gotten from $B = AA^T$. $b_{i,j}$ tells us how many documents word $i$ appears in, that word $j$ also appears in\n",
    "2. The document-by-document matrix, a $m \\times m$ matrix, gotten from $C =A^TA$. $c_{i,j}$ tells us how many words are shared by document $i$ and document $j$\n",
    "\n",
    "If we perform SVD on either of these matrices, we can get a lower-dimensional representation of how our documents or our words relate to one another. Letting us plot which words or documents relate to one another, very useful for large corpuses. Of course, there are a few limitations, among other things, we cannot say anything about words that do not appear in the corpus (outside our vocabulary). Nor can we say anything about the ordering of words, grammar, or the endings of words.\n",
    "\n",
    "---\n",
    "\n",
    "Sentiment analysis for a single document can be deceptively simple to implement:\n",
    "\n",
    "1. For each word in the document, get the sentiment of that specific word (based on a known dictionary of sentiments)\n",
    "2. Add this to a sum\n",
    "\n",
    "That's it. It's also pretty simple mathematically: $S = \\sum_{k=1}^K s_k$ where $s_k$ is simply the sentiment of the k'th word - as mentioned, we find this by a dictionary usually, or a 3'rd party model. \n",
    "\n",
    "To get the arousal score instead, how many \"feelings\" a document uses in total, we simply get the absolute sentiment of each word instead.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:47:03.340203Z",
     "start_time": "2024-10-22T13:47:02.286156Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import seaborn as snb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "snb.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 1 - Bag of Words representation\n",
    "\n",
    "Throughout these exercises, we'll work with the following corpus of 6 paper titles. This is of course, very much a toy example, but it shows the fundamentals well enough before we move unto more complicated datasets the follwing weeks.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our documents - In this case, we don't have any numbers or special characters, which makes preprocessing quite a bit easier...\n",
    "documents = ['Analysis of EEG Signal for the Detection of Brain Abnormalities',\n",
    "            'EEG source imaging assists decoding in a face recognition task',\n",
    "            'Brain Source Localization Using EEG Signal Analysis',\n",
    "            'Deep Neural Networks for Object Detection',\n",
    "            'Imagenet classification with deep convolutional neural networks',\n",
    "            'Very deep convolutional networks for large-scale image recognition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 0 - Basic string manipulation in Python\n",
    "\n",
    "Python does tons of things in ways no other programming language would dream of doing them. For that reason, it's good enough to familiarize yourself with at least some of the basics.\n",
    "\n",
    "Overall, familiarize yourself with the commands and methods, play around with them as you like, but don't get stuck too long here.\n",
    "\n",
    "If you're already familiar with string manipulation in Python, feel free to skip this exercise.\n",
    "\n",
    "#### **0.1 üíª Familiarize yourself with the following ways of manipulating strings in Python. Mess around with the results until you get a feel for what each one does.**\n",
    "\n",
    "Assume all variables are python strings (`<class 'str'>`) unless otherwise specified\n",
    "\n",
    "1. Concatenating two strings, `a` and `b` with `a + b`\n",
    "2. Repeating a string `a` n times with `a * n`\n",
    "3. Converting a string `a` to lower- or upper-case with `a.lower()` and `a.upper`, respectively\n",
    "4. Finding out whether a substring `c` is contained within a given string `a` with `c in a` (yields true or false)\n",
    "5. Splitting a string `a` based on a certain delimiter `d` with `a.split(d)` (default split value is on whitespace)\n",
    "6. Joining a list of strings `e`, while adding a given delimiter `f` between each element with `f.join(e)` \n",
    "7. Use f-strings to format with. Adding a string `b` to a given string `a` being created with the pattern of `a = f\"the value of b will be printed after the colon: {b}\"`\n",
    "\n",
    "Kinda uniquely, Python also lets you treat strings as 'lists of characters', so you can index and slice strings as you would normally\n",
    "\n",
    "8. Index a string `a` to get the `n`'th (where n is an integer) letter with `a[n]`\n",
    "9. Get a substring from `a` from the `n`'th to the `m`'th letter (`n` and `m` being integers): `a[n:m]`\n",
    "10. Get the number of letters in a string `a` with `len(a)`\n",
    "\n",
    "Finally, Python supports [Regular Expressions](https://en.wikipedia.org/wiki/Regular_expression). Essentially a supremely flexible way of matching strings or finding patterns in strings. Python does this through the `re` module, available by default in most Python compilers. re in Python uses '[patterns](https://docs.python.org/3/library/re.html)' to determine what is being matched on. These can be rather complicated, so our advice is, don't worry about learning them by heart. Look up specific usecases on stackexchange or make an LLM write them...\n",
    "\n",
    "11.  Search for the **first match** on a specific pattern `p` in a string `a` with `re.search(p, a)`\n",
    "12.  Return **all non-overlapping matches** for pattern `p` in a string `a` with `re.findall(p, a)`\n",
    "13.  Split a string `a` on a specific pattern `p`p with `re.split(p, a)` - like a more flexible version of `.split()`\n",
    "14.  Substitute a specific pattern `p` with a string `b`' in a string `a` with `re.sub(p, b, a)`\n",
    "\n",
    "\n",
    "There are obviously tons more, but for something like string manipulation, it is often best to just look up solutions for a specific usecase when you need it.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Some\"\n",
    "b = \"Body\"\n",
    "\n",
    "# 1 String concatenation\n",
    "concatenated = a + b\n",
    "print(concatenated)\n",
    "\n",
    "# 2 String multiplication\n",
    "multiplied = a * 5\n",
    "print(multiplied)\n",
    "\n",
    "# 3 Converting upper, lower\n",
    "lowered = a.lower()\n",
    "uppered = a.upper()\n",
    "print(lowered, uppered)\n",
    "\n",
    "# 4 Substring in String\n",
    "some_in_body = a in b\n",
    "some_in_somebody = a in (a + b)\n",
    "print(\"is some in body:\", some_in_body)\n",
    "print(\"is some in somebody:\", some_in_somebody)\n",
    "\n",
    "# 5 Splitting on delimiter\n",
    "text = \"\"\"Somebody once told me the world is once gonna roll me.\n",
    "I ain't the sharpest tool in the shed\"\"\"\n",
    "\n",
    "split_by_whitespace = text.split()\n",
    "print(split_by_whitespace)\n",
    "\n",
    "split_by_newline = text.split(\"\\n\")\n",
    "print(split_by_newline)\n",
    "\n",
    "# 6 Joining with separator\n",
    "separator = \"\" # blank separator\n",
    "joined_text = separator.join(split_by_whitespace)\n",
    "print(joined_text)\n",
    "\n",
    "# 7 F strings\n",
    "what_you_are = \"all star\"\n",
    "what_am_i = f\"Hey now! You're an {what_you_are}, get your game on, go play!\"\n",
    "print(what_am_i)\n",
    "\n",
    "# 8 Indexing letters\n",
    "first_letter = text[0]\n",
    "last_letter = text[-1]\n",
    "some_letter_in_middle = text[10]\n",
    "print(first_letter, last_letter, some_letter_in_middle)\n",
    "\n",
    "# 9 slicing text\n",
    "first_four_letters = text[0:5] # here, you technically don't need to write 0, and you could have written it as text[:5]\n",
    "print(first_four_letters)\n",
    "\n",
    "# 10 number of letters\n",
    "num_letters = len(text)\n",
    "print(num_letters)\n",
    "\n",
    "# 11 Finding the first match\n",
    "pattern = \"once\"\n",
    "first_match = re.search(pattern, text)\n",
    "print(first_match)\n",
    "\n",
    "# 12 Find all matches\n",
    "pattern = \"once\"\n",
    "first_match = re.findall(pattern, text)\n",
    "print(first_match)\n",
    "\n",
    "# Extra, find first and also every word that starts with a capital letter\n",
    "pattern = r'\\b[A-Z][a-z]*\\b' # r-strings here are 'raw' strings, makes it easier to write \\, otherwise Python woudl escape these\n",
    "first_match = re.search(pattern, text)\n",
    "all_matches = re.findall(pattern, text) \n",
    "print(first_match, all_matches)\n",
    "\n",
    "# Extra, instead of finding it as a list, we can also find it as an iterable of all matches\n",
    "iterable_all_matches = re.finditer(pattern, text)\n",
    "\n",
    "# 13 Split on pattern\n",
    "pattern = r'[^a-zA-Z0-9 ]+' # Matches on everything NOT a capital or lower letter from a-z, a number from 0-9, or a whitespace (notice the whitespace at the end of the square brackets)\n",
    "split_by_special_character = re.split(pattern, text)\n",
    "print(split_by_special_character)\n",
    "\n",
    "# 14 Replace on pattern\n",
    "pattern = r'[a-zA-Z0-9 ]+' # Matches on everything THAT IS capital or lower letter from a-z, a number from 0-9, or a whitespace (notice the whitespace at the end of the square brackets)\n",
    "replace_text = \"pickle\"\n",
    "replace_with_text = re.sub(pattern, replace_text, text)\n",
    "print(replace_with_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 1 - Preprocessing and constructing the vocabulary\n",
    "\n",
    "Here, we want to do a number of things\n",
    "\n",
    "1. Get information on the amount of documents and words in the corpus\n",
    "2. Create a vocabulary of all unique words in the corpus\n",
    "3. Remove all non-informative words, I.E. stop words\n",
    "\n",
    "After implementing each function, you can test them using the cell two times below.\n",
    "\n",
    "#### **1.1 üíª Complete the function `get_corpus_info` below to get the number of words and documents in the corpus**\n",
    "\n",
    "#### **1.2 üíª Complete the function `create_vocab` below to get the vocabulary**\n",
    "\n",
    "#### **1.3 üíª Complete the function `remove_stop_words` to remove stop words from the vocabulary**\n",
    "\n",
    "#### **1.4 Consider the current list of stop words; is it good enough? Are there too many or too few? What are the benefits/drawbacks of removing stop words?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_info(corpus):\n",
    "    \"\"\"Get number of words and documents present in the corpus. Also return list of all words present in corpus\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Documents with all lower-case words and no special characters.\n",
    "\n",
    "    Returns:\n",
    "        int: number of documents in the corpus\n",
    "        int: number of words in all the documents of the corpus\n",
    "        list(str): all the words present in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO:\n",
    "        # 1. Get the number of documents\n",
    "        # 2: Get a list of all words in the corpus\n",
    "        # 3: Get number of words in corpus\n",
    "        # 4: Return all 3\n",
    "\n",
    "\n",
    "    return num_documents, num_words, words\n",
    "\n",
    "def create_vocab(all_words):\n",
    "    \"\"\"Create vocabulary based on all words in the corpus\n",
    "\n",
    "    Args:\n",
    "        all_words (list[str]): Unordered list of all words present in the corpus. Words must be lower-case \n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of all unique words present in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO:\n",
    "        # 1. Get all unique words in all_words\n",
    "        # 2: Return these unique words (optionally sort them first)\n",
    "\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def remove_stop_words(vocabulary, stop_words):\n",
    "    \"\"\"Removes stop words present in a list from a given vocabulary\n",
    "\n",
    "    Args:\n",
    "        vocabulary (list[str]): List of all the words in the vocabulary\n",
    "        stop_words (list[str]): List of all stop words to remove from vocabulary\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Vocabulary with stop words removed\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO:\n",
    "        # 1. For each word in vocabulary, remove that word if it is contained in stop words\n",
    "\n",
    "    return updated_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower-case documements\n",
    "documents = [doc.lower() for doc in documents]\n",
    "\n",
    "num_docs, num_words, words = get_corpus_info(documents)\n",
    "\n",
    "print(f\"Number of documents:\\t{num_docs}\")\n",
    "print(f\"Number of words:\\t{num_docs}\")\n",
    "\n",
    "vocabulary = create_vocab(words)\n",
    "print(f\"Vocabulary size:\\t{len(vocabulary)}\")\n",
    "print(f\"All words in vocabulary: {vocabulary}\")\n",
    "\n",
    "# create list of stop words\n",
    "stop_words = ['the', 'of', 'in', 'via', 'for', 'a', 'with', 'to', 'and', 'very', 'using']\n",
    "print(f\"Number of stop words:\\t{len(stop_words)}\")\n",
    "\n",
    "# Remove stop words from vocabulary\n",
    "vocabulary = remove_stop_words(vocabulary, stop_words)\n",
    "print(f\"Number of words in vocabulary after removing stop words:\\t{len(vocabulary)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 2 - **Constructing the bag-of-words representation**\n",
    "\n",
    "We are now ready to construct the bag-of-words representation for our small text corpus. The bag-of-words model is a matrix representation of a text corpus describes how many times a given word is present in a given document. Specifically, the bag-of-words matrix is a matrix $\\mathbf{A}$, where the number of rows is equal to the number of words in the vocabulary and the number of columns is equal to the number of documents, i.e. $\\mathbf{A} \\in \\mathbb{R}^{\\text{Voculary size} \\times \\text{Number of documents}}$ . The element in the i'th row and j'th column describes how many times the i'th word in the vocabulary occurs in the j'th document\n",
    "\n",
    "$$A_{ij} = \\text{Number of times the i'th word in the vocabulary occurs in the j'th document}.$$\n",
    "\n",
    "It is common that each document only contains a small subset of the vocabulary words, and therefore the bag-of-words matrices are most often **sparse**, meaning that each column contains mostly zeros.\n",
    "\n",
    "\n",
    "The words in the vocabulary is sometimes also referred to as *terms* and the bag-of-words matrix is also referred to as the **term-by-document matrix**.\n",
    "\n",
    "#### **2.1 Inspect the function `construct_bag_of_words` that implements the function for constructing a bag-of-words representation based on a corpus of documents and its associated vocabulary. In broad strokes, how does the function work?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$   \n",
    "\n",
    "</span>\n",
    "\n",
    "#### **2.2 Inspect the output of the function by running the cell two steps below. Is there anything that stands out in this particular BoW?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:47:12.416204Z",
     "start_time": "2024-10-22T13:47:12.393649Z"
    }
   },
   "outputs": [],
   "source": [
    "def construct_bag_of_words(documents, vocabulary):\n",
    "    '''\n",
    "    This functions takes a list of documents and a vocabulary a returns the corresponding bag-of-words representation of the documents\n",
    "    \n",
    "    If the variable \"document\" is a list of N documents and the variable \"vocabulary\" is a list of M words, then the function should return a bag-of-words matrix A of size N x M matrix.\n",
    "    If some word from a document is not present in the vocabulary, it should be ignored.\n",
    "    '''\n",
    "    \n",
    "    num_doc = len(documents)\n",
    "    vocab_size = len(vocabulary)\n",
    "        \n",
    "    # Make a word-to-idx mapping so each word can be mapped to a unique index\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    \n",
    "    # Create a matrix where rows are words and columns are documents\n",
    "    bag_of_words = np.zeros((vocab_size, num_doc))\n",
    "    \n",
    "    # Enumerate over each document\n",
    "    for idx_doc, doc in tqdm(enumerate(documents)):\n",
    "        # And each word in document...\n",
    "        for word in doc.split():\n",
    "            if word in vocabulary:\n",
    "                # ...increment the counter for that one word in a given document once (use word2idx to get a index based)\n",
    "                bag_of_words[word2idx[word], idx_doc] += 1\n",
    "\n",
    "    return bag_of_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:47:13.877657Z",
     "start_time": "2024-10-22T13:47:13.855659Z"
    }
   },
   "outputs": [],
   "source": [
    "A = construct_bag_of_words(documents, vocabulary)       \n",
    "\n",
    "# print bag of words matrix and along with the vocabulary\n",
    "print(35*'-')\n",
    "print('Bag of word matrix')\n",
    "print(35*'-')\n",
    "\n",
    "for vocab_word, A_row in zip(vocabulary, A):\n",
    "    # The :<15 means we left-align the text with 15 characters width\n",
    "    print(f\"{vocab_word:<15} {A_row}\")\n",
    "    \n",
    "# print documents\n",
    "print('\\n')\n",
    "print(70*'-')\n",
    "print('Documents')\n",
    "print(70*'-')\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 3 - Examining output from the BoW representation\n",
    "\n",
    "Here, we consider two new \"documents\":\n",
    "\n",
    "- \"EEG Source localization assists face recognition\"\n",
    "- \"Face recognition assists EEG Source localization\"\n",
    "\n",
    "And examine how they are represented in our BoW\n",
    "\n",
    "#### **3.1 üíª Write code to create a BoW representation of these two new documents. Feel free to use functions and code you've written beforehand**\n",
    "\n",
    "#### **3.2 Using this code, validate the bag-of-words matrix above by comparing it to the documents given here, does it make sense to use our BoW for these new documents? Why / Why not?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **3.3 How could you go about handling the case where a new document came with a word outside of our vocabulary?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **3.4 Explain which semantic structures of the text documents are preserved and which structures are lost using the bag-of-words representation?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:48:34.376220Z",
     "start_time": "2024-10-22T13:48:34.366226Z"
    }
   },
   "outputs": [],
   "source": [
    "document_1 = \"EEG Source localization assists face recognition\"\n",
    "document_2 = \"Face recognition assists EEG Source localization\"\n",
    "\n",
    "# TODO:\n",
    "    # 1. Use the construct_bag_of_words function wiht the new documnets and existing vocabulary\n",
    "\n",
    "# Create bag of words\n",
    "A_1 =\n",
    "\n",
    "for vocab_word, A_row in zip(vocabulary, A_1):\n",
    "    # The :<15 means we left-align the text with 15 characters width\n",
    "    print(f\"{vocab_word:<15} {A_row}\")\n",
    "\n",
    "# print documents\n",
    "print('\\n')\n",
    "print(70*'-')\n",
    "print('Documents')\n",
    "print(70*'-')\n",
    "for doc in [document_1, document_2]:\n",
    "    print(doc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 4 - The term-by-term and document-by-document matrices\n",
    "\n",
    "*We will now construct the so-called term-by-term matrix, $\\mathbf{B} = \\mathbf{A}\\mathbf{A}^T$, and document-by-document matrix, $\\mathbf{C}=\\mathbf{A}^T\\mathbf{A}$. The $\\mathbf{B}$ matrix is sometimes also known as the **co-occurence matrix**.*\n",
    "\n",
    "\n",
    "#### **4.1. Explain why these names are appropriate for the matrices $\\mathbf{B}$ and $\\mathbf{C}$**\n",
    "\n",
    "*Hint: What does an inner product between two vectors represent?*\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.2 Run the code in the cell below and inspect the plot of the matrix $\\mathbf{A}$ to figure how many times the word \"recognition\" is present in the corpus.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.3 What is the interpretation of the diagonal of the term-by-term matrix $\\mathbf{B}$?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.4. Inspect the plot of the matrix $\\mathbf{A}$ again to figure how many words the first document contains.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.5 What is the interpretation of the diagonal of the document-by-document matrix $\\mathbf{C}$?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.6. What does it mean of an off-diagonal element in $C_{n,m}$, $n \\neq m$ is zero?**\n",
    "*Hint: The (n,m)'th element of $\\mathbf{C}$ can be written as: $C_{nm} = \\sum_{k} A_{kn} A_{km}$ and $A_{kn}$ is zero if the k'th word is not present in the m'th document*\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:46:54.391940Z",
     "start_time": "2024-10-22T13:46:53.084266Z"
    }
   },
   "outputs": [],
   "source": [
    "#compute the B and C matrix\n",
    "B = np.matmul(A, A.T) # Also possible to use A @ A.T\n",
    "C = np.matmul(A.T, A) # Also possible to use A.T @ A\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "im = plt.imshow(A)\n",
    "plt.xlabel('Doc')\n",
    "plt.yticks(np.arange(0, len(vocabulary)), vocabulary)\n",
    "plt.title('$A$ (Term x document)')\n",
    "plt.grid(False)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"15%\", pad=0.1)\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "im = plt.imshow(B)\n",
    "plt.xlabel('Terms')\n",
    "plt.ylabel('Terms')\n",
    "plt.title('$B = AA^T$ (Term x Term)')\n",
    "plt.grid(False)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "im = plt.imshow(C)\n",
    "plt.xlabel('Doc')\n",
    "plt.ylabel('Doc')\n",
    "plt.title('$C = A^T A$ (Document x Document)')\n",
    "plt.grid(False)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 5: Another interpretation of the matrices $B$ and $C$\n",
    "\n",
    "If we consider each document as a separate observation, then the matrix $\\mathbf{B}$ can be interpreted as the second moments of the documents and hence, the matrix $\\mathbf{B}$ is closely related to the empirical covariance matrix $\\mathbf{S}$ of the documents. This naturally suggests that we use principal component analysis (PCA) to understand the structure of the documents by decomposing the matrix $\\mathbf{B}$ into eigenvectors and eigenvalues,\n",
    "\n",
    "$\\mathbf{B} = \\mathbf{U}\\Lambda  \\mathbf{U}^\\top$,\n",
    "\n",
    "where $\\mathbf{U}$ is a matrix of eigenvectors and $\\Lambda$ is a diagonal matrix of eigenvalues.  Specifically, it is a basis matrix where each basis direction is a latent topic ‚Äì you can think of each as a ‚Äútheme‚Äù capturing a particular pattern of how terms co-occur across documents.\n",
    "\n",
    "Using a similar line of reasoning, we can decompose $\\mathbf{C}$ as follows\n",
    "\n",
    "$\\mathbf{C} = \\mathbf{V}\\Lambda  \\mathbf{V}^\\top$.\n",
    "\n",
    "It can be shown that the two matrices $\\mathbf{B}$ and $\\mathbf{C}$ share the same eigenvalues, so we only need one common $\\Lambda$ matrix.\n",
    "\n",
    "From $\\mathbf{B}\\mathbf{x} = \\lambda \\mathbf{x}$ we can obtain $\\mathbf{C}\\mathbf{z} = \\lambda \\mathbf{z}$ by using the definitions of $\\mathbf{B}$ and $\\mathbf{C}$.\n",
    "That is, $\\lambda$ is an eigenvalue of $\\mathbf{C}$ corresponding to the eigenvector $\\mathbf{A}^T \\mathbf{x}$.\n",
    "\n",
    "*(Optional) Technical note: Formally, the term-by-term matrix $\\mathbf{B}$ is the second order moment of the documents and not the covariance matrix as typically used in principal component analysis. We could obtain the actual empirical covariance matrix from by substracting the outer product of the mean vectors, i.e. $\\mathbf{S} = \\mathbf{B} - \\mathbf{m}\\mathbf{m}^T$, where $\\mathbf{m}$ is the mean document vector. However, we choose not to do this for computational reasons. Namely, the matrix $\\mathbf{B}$ is sparse, meaning that most of the entries are equal to zero. This has the benefit that we can represent it very efficiently in our computers. In contrast, the covariance matrix $\\mathbf{S}$ will in general not be sparse and hence, it is requires more memory to store the matrix. This does not make a big difference for the small example we consider here, but this can be prohibitive costly for real-world large-scale applications.  The same line of reasoning applies to the matrix $\\mathbf{C}$.*\n",
    "\n",
    "#### **5.1. Run the code below and verify that the eigenvalues of the two matrices are indeed the same:**\n",
    "\n",
    "#### **5.2. Seemingly, B has more eigenvalues than C, yet these are all 0, why is this the case?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:41:45.108286Z",
     "start_time": "2024-10-17T08:41:45.093170Z"
    }
   },
   "outputs": [],
   "source": [
    "def eigendecompose(X):\n",
    "    ''' Return the eigendecomposition (E, V) of X s.t. eigenvalues are sorted descendingly '''\n",
    "    lam, V = np.linalg.eigh(X)\n",
    "    sort_idx = np.argsort(lam)[::-1]\n",
    "    return lam[sort_idx], V[:, sort_idx]\n",
    "\n",
    "lambdas_C, V = eigendecompose(C)\n",
    "lambdas_B, U = eigendecompose(B)\n",
    "# Pretty print with 4 decimal places\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "# Print for lambdas_C\n",
    "print(\"Eigenvalues of C:\")\n",
    "print(lambdas_C)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print for lambdas_B\n",
    "print(\"Eigenvalues of B:\")\n",
    "print(lambdas_B)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the difference between lambdas_C and the first 6 eigenvalues of B\n",
    "print(\"Difference (lambdas_C - first 6 of lambdas_B):\")\n",
    "print(lambdas_C - lambdas_B[:len(lambdas_C)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 6: Latent semantic analysis - Document-by-document\n",
    "\n",
    "Studying the latent spaces induced by the eigendecompositions above is known as **Latent Semantic Analysis (LSA)** or sometimes also known as **Latent Semantic Indexing (LSI)**. We will now project each document to the latent semantic space to analyze how the documents relate to each other. \n",
    "\n",
    "Recall that,\n",
    "\n",
    "$\\mathbf{A} = \\mathbf{U}\\Lambda^{\\frac{1}{2}}  \\mathbf{V}^T$\n",
    "\n",
    "$\\mathbf{B} = \\mathbf{A}\\mathbf{A}^\\top = \\mathbf{U}\\Lambda^{\\frac{1}{2}} \\underbrace{\\mathbf{V}^T    \\mathbf{V}}_{\\mathbb{I}_{M\\times M}} \\Lambda^{\\frac{1}{2}} \\mathbf{U}^\\top = \\mathbf{U}\\Lambda \\mathbf{U}^\\top$\n",
    "\n",
    "$\\mathbf{C} = \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{V}\\Lambda^{\\frac{1}{2}} \\underbrace{\\mathbf{U}^T    \\mathbf{U}}_{\\mathbb{I}_{N\\times N}} \\Lambda^{\\frac{1}{2}} \\mathbf{V}^\\top = \\mathbf{V}\\Lambda \\mathbf{V}^\\top$\n",
    "\n",
    "We can project each document in our corpus to the latent semantic space as follows\n",
    "\n",
    "$\\mathbf{z}_{\\text{doc}} = \\mathbf{\\Lambda}_k^{-\\frac12}\\mathbf{U}_k^T \\mathbf{A} = \\mathbf{P}_k \\mathbf{A}$\n",
    "\n",
    "where $\\mathbf{P}_k = \\mathbf{\\Lambda}_k^{-\\frac12}\\mathbf{U}_k^T $ acts as a projection matrix. \n",
    "\n",
    "$\\mathbf{P}_k = \\mathbf{\\Lambda}_k^{-\\frac12}\\mathbf{U}_k^T$\n",
    "\n",
    "For visualization purposes, we will focus on $k = 2$ dimensions for now.\n",
    "\n",
    "#### **6.1. Inspect the code below and list roughly what it does in order**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **6.2. The figure produced, shows that the 6 documents form 2 clusters in the latent semantic space. Can you figure out what the papers in each cluster have in common?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:51:33.991658Z",
     "start_time": "2024-10-17T08:51:33.723162Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's focus on the first two components for visualization purposes\n",
    "k = 2\n",
    "\n",
    "# Eigenvectors of document-by-document\n",
    "Uk = U[:, :k]\n",
    "\n",
    "# Eigenvalues\n",
    "Lk = lambdas_C[:k]\n",
    "\n",
    "# Eigenvectors of term-by-term\n",
    "Vk = V[:, :k]\n",
    "\n",
    "# define projection matrix for the documents\n",
    "P = np.diag(1./np.sqrt(Lk))@Uk.T\n",
    "\n",
    "# project\n",
    "z_doc = P@A\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "for idx, doc in enumerate(z_doc.T):\n",
    "    plt.plot([0, doc[0]], [0, doc[1]], 'b--')\n",
    "    circle = plt.Circle((doc[0], doc[1]), radius=0.015)\n",
    "    ax.add_patch(circle)\n",
    "    label = ax.annotate(idx, xy=(doc[0], doc[1]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.xlabel('Latent Semantic dim 1')\n",
    "plt.ylabel('Latent Semantic dim 2')\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis('off')\n",
    "for idx, doc in enumerate(documents):\n",
    "    i = idx\n",
    "    plt.text(0, 1 - i / len(vocabulary), 'Doc %d: %s' % (i, doc), color='b')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The dashed lines help to emphasise the angles between the vectors, which will be relevant when we use the cosine similarity metric later.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 7: Latent semantic analysis - Word-by-word\n",
    "\n",
    "We can also project each word in our vocabulary to a 2 dimensional latent semantic space as follows\n",
    "\n",
    "$\\mathbf{z}_{\\text{word}} = \\mathbf{A} \\mathbf{V}_k  \\mathbf{\\Lambda}_k^{-\\frac12} = \\mathbf{A}  \\mathbf{P}_k$\n",
    "\n",
    "where $\\mathbf{P}_k = \\mathbf{V}_k  \\mathbf{\\Lambda}_k^{-\\frac12}$ acts as a projection matrix. \n",
    "\n",
    "#### **7.1. Explain The clustering structure that emerges in the semantic space**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **7.2. Two words appear to be between the two clusters. Can you explain why?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:51:35.301074Z",
     "start_time": "2024-10-17T08:51:34.521235Z"
    }
   },
   "outputs": [],
   "source": [
    "# define projection matrix for the words\n",
    "P = Vk@np.diag(1./np.sqrt(Lk))\n",
    "\n",
    "# project\n",
    "z_word = A@P\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "for idx, term in enumerate(z_word):\n",
    "    plt.plot([0, term[0]], [0, term[1]], 'r--')\n",
    "    circle = plt.Circle((term[0], term[1]), radius=0.015, color='r')\n",
    "    ax.add_patch(circle)\n",
    "    label = ax.annotate(idx, xy=(term[0], term[1]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "    \n",
    "plt.xlabel('Latent Semantic dim 1')\n",
    "plt.ylabel('Latent Semantic dim 2')\n",
    "    \n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.axis('off')\n",
    "for idx, term in enumerate(vocabulary):\n",
    "    i = idx\n",
    "    plt.text(0.85, 1 - i/len(vocabulary), 'Term %d: %s' % (i, term), color='r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 8: Document retrieval: Let's try to find all the documents that have to do with the word \"image\" using cosine similarity\n",
    "\n",
    "So far we have seen how we can relate words and documents to each other by project them to the common latent semantic space. We will now make use of these properties in order to build a simple **document retrival** system to search for relevant documents.\n",
    "\n",
    "We will use the following recipe for searching for documents:\n",
    "1. First, we will define a query vector with containing the keywords (using words from the vocabulary)  that we want to use in our search.\n",
    "2. Next, we construct a bag-of-words representation of this vector using the function we implemented above. \n",
    "3. We project the bag-of-words representation to the latent semantic space.\n",
    "4. We loop through the documents in the corpus to find the document that are closets to our projected query vector.\n",
    "5. Return a list of the best matching documents\n",
    "\n",
    "In bullet 4 above, we need to find the \"closest\" documents. There are several ways to compare documents, but here we will use the *cosine similarity* metric:\n",
    "\n",
    "$\\text{cosine similarity} (\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x}^T \\mathbf{y}}{\\|\\mathbf{x}\\|\\, \\| \\mathbf{y} \\|}$\n",
    "\n",
    "#### **8.1. üíª Implement the function `cos_sim` for the system described above, and explain what the rest of the code below does:** \n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **2. Explain why the third document is almost as relevant to the search query as the first two documents even though the third document does not contain the word \"Image\"?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:51:37.234832Z",
     "start_time": "2024-10-17T08:51:37.222501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cos_sim(x, y):\n",
    "    \"\"\"Given two vectors, get the cosine similarity of them\n",
    "\n",
    "    Args:\n",
    "        x np.ndarray: First vector to compare to second \n",
    "        y np.ndarray: Second vector to compare to first\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Single dimensional numpy array containing the cosine similarity between x and y\n",
    "    \"\"\"\n",
    "    # Return cosine similarity here\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:53:00.681375Z",
     "start_time": "2024-10-17T08:53:00.666525Z"
    }
   },
   "outputs": [],
   "source": [
    "# define search query\n",
    "query = ['image']\n",
    "    \n",
    "# construct bag of words presentation for query\n",
    "q = construct_bag_of_words(query, vocabulary)\n",
    "\n",
    "# compute projection matrix\n",
    "P = np.diag(1./np.sqrt(Lk))@Uk.T\n",
    "\n",
    "# project\n",
    "q_proj = P@q\n",
    "\n",
    "# make sure that q_proj has the right shape\n",
    "assert q_proj.shape == (2, 1), \"The shape of q_proj must be (2, 1)\"\n",
    "\n",
    "similarities = np.array([cos_sim(q_proj, doc.T) for doc in z_doc.T])\n",
    "\n",
    "doc_sims = zip(documents, similarities)\n",
    "sorted_docs = sorted(doc_sims, key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print('Sorted list of document related to:', query)\n",
    "print(100*'-')\n",
    "for rank, doc_sim in enumerate(sorted_docs):\n",
    "    doc, sim = doc_sim\n",
    "    print(f'{rank + 1}. {doc:<70} ({sim.item():.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 9: Topics\n",
    "\n",
    "Sometimes it is possible to interpret each of the semantic latent dimensions as topic. Below we will analyze which words from the vocabulary have the strongest connection to the latent dimensions\n",
    "\n",
    "#### **9.1. The figure created from the code below shows the coefficient associated with each word for each topic. Which words are positively associated with topic 2 and which words are negatively associated with topic 2?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:37:19.486379Z",
     "start_time": "2024-10-17T08:37:19.153632Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "ax = plt.gca()\n",
    "im = plt.imshow(Uk.T, cmap=plt.cm.RdBu_r)\n",
    "plt.grid(False)\n",
    "plt.title('Transpose of the matrix $\\mathbf{U}_k$')\n",
    "plt.xticks(np.arange(0, len(vocabulary)), vocabulary, rotation=90)\n",
    "plt.yticks(np.arange(0, k), ['Topic %d' % (i+1) for i in range(k)])\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "term_per_topic =5\n",
    "for i, topic in enumerate(Uk.T):\n",
    "    \n",
    "    terms_topic = zip(vocabulary, topic)\n",
    "    sorted_terms = sorted(terms_topic, key= lambda x:x[1], reverse=True)[:term_per_topic]\n",
    "    \n",
    "    print(\"Topic %d (latent dimension %i)\" % (i+1, i+1))\n",
    "    for t in sorted_terms:\n",
    "        print('%20s (%3.2f)' % t)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 2: Solving odd-one-out\n",
    "\n",
    "As an example, find the 'odd-one-out' from the the list of four words: 'ship', 'car', 'plane', and 'orange'. \n",
    "\n",
    "Clearly, 'orange' is the odd one out here. In this exercise we will use latent semantic analysis to build a system to solve these puzzles automatically. \n",
    "\n",
    "Specifically, we will analyse 8447 articles from the News York Times (available here: https://github.com/moorissa/nmf_nyt), learn a latent semantic space and use that space to the solve odd-one-out puzzles.\n",
    "\n",
    "First, we will load the data. The vocabulary list and the bag-of-words matrix are already prepared for us.\n",
    "\n",
    "**You don't need to get all of the below code, but give it a read and see if it makes sense**\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:57:09.308979Z",
     "start_time": "2024-10-17T08:57:08.342456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open the data which is essentially one big tokenized document set\n",
    "with open('data/nyt_data.txt') as f:\n",
    "    documents = f.readlines()\n",
    "documents = [x.strip().strip('\\n').strip(\"'\") for x in documents] \n",
    "\n",
    "# Oben the vocabulary where the numbers correspond to what words are present in the documents\n",
    "with open('data/nyt_vocab.dat') as f:\n",
    "    vocabulary = f.readlines()\n",
    "vocabulary = [x.strip().strip('\\n').strip(\"'\") for x in vocabulary] \n",
    "\n",
    "num_doc = 8447\n",
    "vocab_size = 3012 \n",
    "A = np.zeros([vocab_size,num_doc])\n",
    "\n",
    "for col in range(len(documents)):\n",
    "    for row in documents[col].split(','):\n",
    "        A[int(row.split(':')[0])-1,col] = int(row.split(':')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "**1. Construct the term-by-term matrix $\\mathbf{B} = \\mathbf{A}\\mathbf{A}^T$ and document-by-document $\\mathbf{C} = \\mathbf{A}^T \\mathbf{A}$ and decompose them to eigenvalues and eigenvectors.**\n",
    "\n",
    "**Be patient, this might take up to a few minutes.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:38:18.239377Z",
     "start_time": "2024-10-17T08:37:20.103340Z"
    }
   },
   "outputs": [],
   "source": [
    "B = A@A.T\n",
    "C = A.T@A\n",
    "\n",
    "lambdas, U = eigendecompose(B)\n",
    "print('Done decomposing B.')\n",
    "\n",
    "lambdas, V = eigendecompose(C)\n",
    "print('Done decomposing C.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "**2. Select the first $k$ dimensions and project the words that the $k$-dimensional latent space**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:38:18.240515Z",
     "start_time": "2024-10-17T08:38:18.240515Z"
    }
   },
   "outputs": [],
   "source": [
    "k = 20\n",
    "Uk = U[:, :k]\n",
    "Sk = lambdas[:k]\n",
    "Vk = V[:, :k]\n",
    "\n",
    "# define projection matrix for the words\n",
    "P = Vk@np.diag(1./np.sqrt(Sk))\n",
    "\n",
    "# project\n",
    "z_word = A@P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the list of odd-one-out puzzles that we want to solve using latent semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:38:18.242852Z",
     "start_time": "2024-10-17T08:38:18.241540Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_puzzles = [\n",
    "                    ['ship', 'car', 'plane', 'orange'],\n",
    "                    ['man', 'car', 'woman', 'person'],\n",
    "                    ['sky', 'cup', 'glass', 'bowl'],\n",
    "                    ['black', 'shoe', 'blue', 'red'],\n",
    "                    ['basketball', 'coffee', 'water', 'milk'],\n",
    "                    ['telephone', 'computer', 'dog', 'radio'],\n",
    "                    ['president', 'door', 'minister', 'secretary'],\n",
    "                    ['soccer', 'basketball', 'tennis', 'rain'],\n",
    "                    ['photography', 'image', 'painting', 'airplane']\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 10: Odd-one-out - Solving odd-one-out puzzles using 8447 articles from the New York Times\n",
    "\n",
    "The code below implements the odd-one-out solver.\n",
    "\n",
    "#### **10.1. Study the function `find_odd_one_out` below and explain what it does.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **10.2. Add some puzzles of your own to the above list. Can you create puzzles that 'tricks' the solver?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **10.3. Experiment with the number of latent dimensions k. How many dimensions do we need to solve this odd-one-out puzzles?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:38:18.245206Z",
     "start_time": "2024-10-17T08:38:18.244170Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_odd_one_out(puzzle):\n",
    "    ''' the function takes a list of 4 words and \n",
    "        1. constructs a bag-of-words representation for each word\n",
    "        2. projects them to the latent semantic space\n",
    "        3. identifies the odd-one-out using cosine similaries\n",
    "        4. returns the odd word '''\n",
    "    \n",
    "    # check if all words are present in vocab\n",
    "    for word in puzzle:\n",
    "        if word not in vocabulary:\n",
    "            print('Word \"%s\" not found in vocabulary' % word)\n",
    "    \n",
    "    # construct bag-of-words representation\n",
    "    q = construct_bag_of_words(puzzle, vocabulary)\n",
    "    \n",
    "    # compute project matrix\n",
    "    P = np.diag(1./np.sqrt(Sk))@Uk.T\n",
    "\n",
    "    # project to semantic space\n",
    "    semantic_vectors = P@q\n",
    "\n",
    "    # for each vector compute average cosine similarities to the three other concepts\n",
    "    avg_sim = []\n",
    "    for i in range(len(semantic_vectors.T)):\n",
    "\n",
    "        # compute cosine similarities between i and the remaining three vectors\n",
    "        cos_similarities = [cos_sim(semantic_vectors[:, i], semantic_vectors[:, j]) for j in range(len(semantic_vectors.T)) if i != j]\n",
    "\n",
    "        # compute average and store\n",
    "        avg_sim.append(np.mean(cos_similarities))\n",
    "        \n",
    "    # return the concept with lowest average cosine similarity\n",
    "    return puzzle[np.argmin(avg_sim)]\n",
    "\n",
    "#use the find_odd_one_out function to solve the puzzles\n",
    "for puzzle in list_of_puzzles:\n",
    "    \n",
    "    odd_one = find_odd_one_out(puzzle)\n",
    "    \n",
    "    print('Concepts:', ', '.join(puzzle))\n",
    "    print('Odd-one-out: %s' % odd_one)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 3: Sentiment analysis of Amazon reviews\n",
    "\n",
    "In the last part of the exercise, we will use sentiment analysis to Amazon product reviews. \n",
    "\n",
    "**The data (~250MB) can be downloaded from here: https://www.kaggle.com/snap/amazon-fine-food-reviews**\n",
    "\n",
    "The goal is to predict whether the author of the review likes or dislikes the product he or she is reviewing.\n",
    "\n",
    "(You can see more details about the data here if you are interested: https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3)\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T08:38:18.245206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data first\n",
    "\n",
    "df = pd.read_csv('data/Reviews.csv', usecols=['Text', 'Score'])\n",
    "\n",
    "# Let's get rid of the neutral reviews\n",
    "df.dropna(inplace=True)\n",
    "df[df['Score'] != 3]\n",
    "\n",
    "# .. and group negative reviews (rating = 1 or 2) together and group positive reviews (ratings = 4 or 5) together\n",
    "df['Pos'] = np.where(df['Score'] > 3, 1, 0)\n",
    "\n",
    "documents = df['Text'].tolist()\n",
    "positives = df['Pos'].tolist()\n",
    "num_doc = len(documents)\n",
    "\n",
    "print(f\"In total, we have {num_doc} product reviews. Each review has an associated score of either 0 or 1, where 0 means a negative review and 1 means a positive review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "**We then proprocess the text data by converting all reviews to lower case and remove all punctuation (periods, commas, questionmarks and exclamation marks)**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T08:38:18.245206Z"
    }
   },
   "outputs": [],
   "source": [
    "documents_cleaned = []\n",
    "for doc in documents:\n",
    "    \n",
    "    # to lower\n",
    "    doc_cleaned = doc.lower()\n",
    "    \n",
    "    # Potentially remove stop words, not really important...\n",
    "    #doc_cleaned = ' '.join([word for word in doc_cleaned.split() if word not in stop_words])    \n",
    "\n",
    "    # All of these replacements could technically be done with a single re.sub\n",
    "    doc_cleaned = doc_cleaned.replace('.', '')\n",
    "    doc_cleaned = doc_cleaned.replace(',', '')\n",
    "    doc_cleaned = doc_cleaned.replace('?', '')\n",
    "    doc_cleaned = doc_cleaned.replace('!', '')\n",
    "    documents_cleaned.append(doc_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 11: Sentiment analysis using lexicons\n",
    "\n",
    "By 'lexicons', we mean pre-curated lists of words and their attached sentiment\n",
    "\n",
    "Now we will load our sentiment lexicon. We will use Afinn created by Finn Nielsen (https://github.com/fnielsen/afinn)\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "from collections import defaultdict\n",
    "sent_lexicon = defaultdict(lambda: 0, Afinn()._dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T08:38:18.247866Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initially, lets look up the sentiment for a few words\n",
    "list_of_words = ['amazing', 'accept', 'hate', 'dislike', 'appreciate', 'stupid',  'love', 'like', 'enjoy', 'best', 'bad', 'worst', 'outstanding', 'food', 'politics', 'movie']\n",
    "\n",
    "for word in list_of_words:\n",
    "    if word in sent_lexicon:\n",
    "        print('%-15s%+3.2f' % (word, sent_lexicon[word]))\n",
    "    else:\n",
    "        print('Word \"%s\" not found' % word)\n",
    "\n",
    "\n",
    "# And the number of words in the lexicon\n",
    "print(\"Number of words in the lexicon:\", len(sent_lexicon))\n",
    "\n",
    "# And the range of the sentiment values\n",
    "lexicon_values = sorted(set(list(sent_lexicon.values())))\n",
    "print(\"The unique sentiment values are:\", lexicon_values)\n",
    "print(f\"That is, each word in the sentiment lexicon is associated with a score between {min(lexicon_values)} and {max(lexicon_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "#### **11.1 üíª Implement the function `get_review_sentiment` to get the sentiment of a single review. It may be worthwhile to also add the possibilty to print the individual sentiments of each word in the review**\n",
    "\n",
    "\n",
    "#### **11.2 Consider some limitations of this approach to getting sentiments?**\n",
    "*Hint: Can you think of an example, where a single word changes the whole meaning of the review that is not found by the sentiment analysis?*\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T08:38:18.249928Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_review_sentiment(review, v=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        review (str): single review to score based on sentiment analysis\n",
    "        v (bool, optional): Verbose?: Whether to print extended information about the review. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        float: Combined sentiment score of the whole review\n",
    "    \"\"\"\n",
    "    if v:\n",
    "        print(f\"Review to score: {review}\")\n",
    "        print('Words           Sentiments')\n",
    "        print(26*'-')\n",
    "    \n",
    "    sentiment = 0\n",
    "    for word in review.lower().split():\n",
    "        if v:\n",
    "            print('%-15s %3.2f' % (word, sent_lexicon[word]))\n",
    "        sentiment += sent_lexicon[word]\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "sentiment = get_review_sentiment(documents_cleaned[0])\n",
    "\n",
    "print(f\"\\nOverall sentiment: {sentiment:2.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 12 - Sentiment lexicon to vocabulary and sentiment vector\n",
    "\n",
    "In order to use the tools we already have at our disposal, we can convert the sentiment lexicon into a sentiment vocabulary and a sentiment vectors. This will allow us to compute sentiments as inner products between the bag of words representation and the sentiment vector\n",
    "\n",
    "Using the bag of words representation, we can instead compute sentiments using inner products\n",
    "\n",
    "#### **12.1 üíª Implement the function `dot_product_sentiment` to get the sentiment of a single review (or list of reviews) by using the dot product between the review(s) BoW representation and a vector of sentiments**\n",
    "\n",
    "#### **12.2 üíª Use this same function to get a vector of the sentiment of every review in the dataset**\n",
    "\n",
    "#### **12.3 Explain some of the advantages of getting sentiments this way, as opposed to as before in a for loop**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **12.4 To get the sentiments of all reviews, we first construct a BoW for all the reviews, what do you think its shape is?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T08:38:18.254450Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sentiment_vocabulary(sent_lexicon):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sent_lexicon (defaultdict[str: int]): defaultdict of words as keys and sentiments as values. Should return 0 in case of unknown word\n",
    "\n",
    "    Returns:\n",
    "        dict_keys: A dict_keys object of all words in the sentiment lexicon\n",
    "        np.ndarray: A numpy vector where the i'th element corresponds to the sentiment of the i'th word in the sentiment vocabulary also returned\n",
    "    \"\"\"\n",
    "\n",
    "    sentiment_vocabulary = sent_lexicon.keys()\n",
    "    sentiment_vec = np.zeros((len(sentiment_vocabulary), 1))\n",
    "    for idx, word in enumerate(sentiment_vocabulary):\n",
    "        sentiment_vec[idx, 0] = sent_lexicon[word]\n",
    "\n",
    "    return sentiment_vocabulary, sentiment_vec\n",
    "\n",
    "def dot_product_sentiment(documents: list[str], sentiment_vocabulary, sentiment_vec):\n",
    "    \"\"\"Get the sentiment of a str by using the dot product of its BoW representation, a sentiment vocab, and a sentiment vector \n",
    "\n",
    "    Args:\n",
    "        document (list[str]): documents to get the sentiment of\n",
    "        sentiment_vocabulary (dict_keys): A dict_keys object of all words in the sentiment lexicon\n",
    "        sentiment_vec (np.ndarray): A numpy vector where the i'th element corresponds to the sentiment of the i'th word in the sentiment vocabulary also returned\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: vector of sentiments where the i'th element corresponds to the sentiment of the i'th document in the input list\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO:\n",
    "        # 1. Get BoW represenation of the given list of doucments and given sentiment focabulary\n",
    "        # 2. Get the sentiments using this BoW represenntation and a given vector of sentiments of words\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentiment of a single document\n",
    "chosen_document = documents_cleaned[0]\n",
    "\n",
    "# The star on create_sentiment_vocabulary is a smart method of *unpacking* arguments in Python. You can look it up if you want\n",
    "sentiment = dot_product_sentiment([chosen_document], *create_sentiment_vocabulary(sent_lexicon))\n",
    "\n",
    "print(\"Sentiment of the single document:\", sentiment)\n",
    "\n",
    "# Get the sentiment of every document in the reviews\n",
    "print(\"\\nCalculating all sentiments, this may take a while...\")\n",
    "\n",
    "all_sentiments = dot_product_sentiment(documents_cleaned, *create_sentiment_vocabulary(sent_lexicon))\n",
    "\n",
    "\n",
    "print(\"First 10 sentiments:\\n\", all_sentiments[:10])\n",
    "print(\"Shape of all_sentiments, what do you think the dimensions mean?\", all_sentiments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 13 - Predicting sentiments\n",
    "\n",
    "Finally, we ask the question: \"Can we predict the thresholded score of the reviews using our calculated sentiments?\"\n",
    "\n",
    "To perform this exercise, remember a few cells above when we loaded the data, we did this:\n",
    "\n",
    "```python\n",
    "# .. and group negative reviews (rating = 1 or 2) together and group positive reviews (ratings = 4 or 5) together\n",
    "df['Pos'] = np.where(df['Score'] > 3, 1, 0)\n",
    "\n",
    "documents = df['Text'].tolist()\n",
    "positives = df['Pos'].tolist()\n",
    "```\n",
    "\n",
    "\n",
    "#### **13.1 Inspect the confusion matrices below as well as the code that creates them. How should the four numbers above be interpreted?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(all_sentiments.ravel() > 0, np.array(positives) > 0)\n",
    "CM_normalized = CM / CM.sum()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Raw counts\n",
    "snb.heatmap(CM, ax=axes[0], annot=True, cmap=\"Reds\", fmt='d',\n",
    "            xticklabels=['true negative', 'true positive'],\n",
    "            yticklabels=['predicted negative', 'predicted positive'])\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized\n",
    "snb.heatmap(CM_normalized, ax=axes[1], annot=True, cmap=\"Reds\", fmt='.5f',\n",
    "            xticklabels=['true negative', 'true positive'],\n",
    "            yticklabels=['predicted negative', 'predicted positive'])\n",
    "axes[1].set_title('Empirical Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signals-and-data-autumn-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
